{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "from IPython.display import Image\n",
    "#check version pandas\n",
    "\n",
    "# Seaborn is a cool library to use whenever starting data exploration\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 7, 7\n",
    "plt.rc(\"font\", size=14)\n",
    "\n",
    "\n",
    "plt.rc('xtick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('xtick.major', size=8, pad=12)\n",
    "plt.rc('xtick.minor', size=8, pad=12)\n",
    "\n",
    "plt.rc('ytick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('ytick.major', size=8, pad=12)\n",
    "plt.rc('ytick.minor', size=8, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c781b",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "## Table Of Content: <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "* [**Loading the data**](#loading)\n",
    "\n",
    "* [**Clustering methods**](#fifth-bullet)\n",
    "    * [K-means](#Kmeans-bullet)\n",
    "    * [Hierarchichal clustering](#HC-bullet)\n",
    "    \n",
    "\n",
    "* [**Exercise: single cell RNAseq data**](#exo-kmeans-single)\n",
    "* [**Additionnal exercise : mammal sleep data**](#exo-kmeans-mammals)\n",
    "\n",
    "\n",
    "* [**Appendix**](#appendix-bullet)\n",
    "    * [DBSCAN](#DBSCAN-bullet)\n",
    "    * [Gaussian mixtures](#gm-bullet)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55802d8d",
   "metadata": {},
   "source": [
    "# Loading the data <a class=\"anchor\" id=\"loading\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d19d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## phospho-proteomics dataset\n",
    "X_ph = pd.read_csv(\"../data/plank2020_phosphoprot_log10.csv\", index_col=0)\n",
    "\n",
    "# imputation\n",
    "X_ph_positive = X_ph[(X_ph>0).all(axis=1)]\n",
    "n_removed = X_ph.shape[0] - X_ph_positive.shape[0]\n",
    "\n",
    "# normalization\n",
    "pept_intens = X_ph_positive.sum(axis=1)\n",
    "X_ph_norm = X_ph_positive.div(pept_intens, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INTROVERT-EXTRAVERT dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dfie = pd.read_csv( '../data/personality_dataset.csv' )\n",
    "\n",
    "# one-hot encoding\n",
    "dfie_OH = pd.get_dummies( dfie , drop_first=True )\n",
    "\n",
    "## each step in the pipeline is given as a (step name , sklearn object) tuple\n",
    "ppl_ie = Pipeline( [('impute',SimpleImputer(strategy = 'mean' )),\n",
    "                    ('scaler', StandardScaler() ),\n",
    "                    ('pca', PCA() )\n",
    "                   ])\n",
    "\n",
    "\n",
    "## fit the whole pipeline and transform the data\n",
    "xpca = ppl_ie.fit_transform( dfie_OH )\n",
    "dfpca_ie = pd.DataFrame( xpca , columns=[ f\"PC{i}\" for i in range(dfie_OH.shape[1]) ] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54aa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IRIS dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X_iris = pd.DataFrame(iris['data'],columns=iris['feature_names'])\n",
    "\n",
    "X_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79024c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "X_iris_scaled = StandardScaler().fit_transform( X_iris )\n",
    "\n",
    "pca = PCA(n_components=2) #create an empty PCA object\n",
    "X_iris_pca = pca.fit_transform(X_iris_scaled)\n",
    "\n",
    "\n",
    "print(\"explained variance with 2 comonents: {:.2f}%\".format(100* sum(pca.explained_variance_ratio_) ) )\n",
    "sns.scatterplot(x=X_iris_pca[:,0],y=X_iris_pca[:,1],hue = iris['target'] , palette = ['#1b9e77','#d95f02','#7570b3'])\n",
    "plt.xlabel('First Principal Component ({0:.2f}%)'.format(pca.explained_variance_ratio_[0]*100))\n",
    "plt.ylabel('Second Principal Component ({0:.2f}%)'.format(pca.explained_variance_ratio_[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab42f06",
   "metadata": {},
   "source": [
    "# Clustering <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d253b7",
   "metadata": {},
   "source": [
    "\n",
    "Clustering is an unsupervised method that serves many puposes:\n",
    "\n",
    "<ol>\n",
    "<li>Pattern detection: discover hidden patterns in your data</li>\n",
    "<li>Compression: replace all cluster members by one representive</li>\n",
    "<li>Data augmentation: replace noisy feature vectors by the cluster consensus vector</li>\n",
    "<li>Dimensionality reduction: replace a feature vector by cluster similarities</li>\n",
    "<li>Outlier detection: detect feature vectors far away from cluster centers</li>\n",
    "<li>Semisupervised learning: assign cluster labels to unlabeled cluster members</li>\n",
    "<li>...</li>\n",
    "</ol>\n",
    "\n",
    "If you find some cluster-like structures in your data and if they correspond to one of your hypothesis then you already came a long way. Rarely you can get there in one step, but you need to carefully clean and normalize your data to discover these structures. Be aware that clustering algorithms are not magical techniques that give you a straight answer: you will need to find good hyperparameters for the model to work, and your expertise and understanding of the data is crutial to obtain good results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d3041",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc) \n",
    "\n",
    "## K-means clustering <a class=\"anchor\" id=\"Kmeans-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95be83c",
   "metadata": {},
   "source": [
    "The [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) algorithm is a widely used clustering algorithm. The K-means algorithm aims at finding the right partition of the data ($\\pmb{X_1},...,\\pmb{X_n}$) into $K$ clusters $\\pmb{C}={C_1,...,C_K}$ so that the within cluster sum of squares (inertia) is minimized:\n",
    "\n",
    "$\\pmb{C}=argmin_{\\pmb{C}} \\sum_{k=1}^{K}\\sum_{i \\in C_k}||\\pmb{X_i}-\\pmb{\\mu_k}||^2$\n",
    "\n",
    "$\\pmb{\\mu_k}=\\frac{1}{|C_k|}\\sum_{i \\in C_k}\\pmb{X_i}$\n",
    "\n",
    "K-means requires the number of clusters as input. The greedy algorithm consists of 3 main parts:\n",
    "<ol>\n",
    "<li>Choose $K$ points from $\\pmb{X_i}$ as initial values for $\\pmb{\\mu_k}$. These initial points should be spread out over the dataset</li>\n",
    "<li>Compute the centroids $\\pmb{\\mu_k}=\\frac{1}{|C_k|}\\sum_{i \\in C_k}\\pmb{X_i}$</li>\n",
    "<li>Assignment step: assign each $\\pmb{X_i}$ to a cluster $C_k=argmin_{C_k} ||\\pmb{X_i}-\\pmb{\\mu_k}||^2$ </li>\n",
    "<li>Update step: recalculate $\\pmb{\\mu_k}=\\frac{1}{|C_k|}\\sum_{i \\in C_k}\\pmb{X_i}$</li>\n",
    "<li>Iterate steps 3 & 4 until convergence</li>\n",
    "</ol>\n",
    "\n",
    "<!-- ![km1](../images/Kmeans1.png) -->\n",
    "\n",
    "<!-- ![km2](../images/Kmeans2.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "# the following code is inpired from https://mlcourse.ai/book/topic07/topic7_pca_clustering.html\n",
    "\n",
    "def Kmean_plot( X , clusters , centroids=None , ax = None):\n",
    "    palette = ['xkcd:brick','xkcd:orange','xkcd:pink','xkcd:silver','xkcd:mustard','xkcd:black'][:len( set(clusters) )]\n",
    "    sns.scatterplot(x=X.iloc[:,0],y=X.iloc[:,1],\n",
    "                    hue = clusters , palette = palette,alpha = 0.5 , ax=ax , legend=False)\n",
    "    if not centroids is None:\n",
    "        sns.scatterplot(x=centroids[:,0],\n",
    "                y=centroids[:,1],\n",
    "                hue = np.arange(centroids.shape[0]), palette = palette , \n",
    "                        s = 300 , marker = 'X' , legend = False , ax=ax)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris_pca = pd.DataFrame( X_iris_pca )\n",
    "\n",
    "K = 3\n",
    "n = X_iris_pca.shape[0]\n",
    "\n",
    "fig,ax = plt.subplots( 2,2 , figsize = (8,8) )\n",
    "\n",
    "\n",
    "\n",
    "## 1. initialization : randomly assign clusters\n",
    "cluster_assignment = np.random.choice(K , n)\n",
    "\n",
    "Kmean_plot( df_iris_pca , cluster_assignment , centroids=None , ax = ax[0,0])\n",
    "ax[0,0].set_title( \"1. random assignment\" )\n",
    "\n",
    "## 2. compute the centroids of each created cluster\n",
    "df_centroids = np.array( df_iris_pca.groupby(cluster_assignment).mean() )\n",
    "\n",
    "Kmean_plot( df_iris_pca , cluster_assignment , centroids=df_centroids , ax = ax[0,1])\n",
    "ax[0,1].set_title( \"2. centroid computation\" )\n",
    "\n",
    "## 3. re-assign each point according ot the closest centroid\n",
    "distances = cdist(df_iris_pca , df_centroids)\n",
    "cluster_assignment = distances.argmin( axis = 1 )\n",
    "\n",
    "Kmean_plot( df_iris_pca , cluster_assignment , centroids=df_centroids , ax = ax[1,0])\n",
    "ax[1,0].set_title( \"3. points assigned\" )\n",
    "\n",
    "\n",
    "## 4. re-compute the centroids of each created cluster\n",
    "df_centroids = np.array( df_iris_pca.groupby(cluster_assignment).mean() )\n",
    "\n",
    "Kmean_plot( df_iris_pca , cluster_assignment , centroids=df_centroids , ax = ax[1,1])\n",
    "ax[1,1].set_title( \"4. centroid updated\" )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9506f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots( 2,2 , figsize = (8,8) )\n",
    "Kmean_plot( df_iris_pca , cluster_assignment , centroids=df_centroids , ax = ax[0,0])\n",
    "ax[0,0].set_title('iteration 0')\n",
    "\n",
    "for i in range( 1,4 ):\n",
    "    ## 3. re-assign each point according ot the closest centroid\n",
    "    distances = cdist(df_iris_pca , df_centroids)\n",
    "    cluster_assignment = distances.argmin( axis = 1 )\n",
    "\n",
    "    ## 4. re-compute the centroids of each created cluster\n",
    "    df_centroids = np.array( df_iris_pca.groupby(cluster_assignment).mean() )\n",
    "\n",
    "    Kmean_plot( df_iris_pca , cluster_assignment , centroids=df_centroids , ax = ax[i//2,i%2])\n",
    "    ax[i//2,i%2].set_title(f'iteration {i}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66732b5d",
   "metadata": {},
   "source": [
    "For data with a cluster structure K-means complexity is often linear in the number of datapoints $\\pmb{X_i}$ and it requires little additional memory. The data can be split into different chunks and the algo still works. Faster versions of the basic algorithm are available, e.g. [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans). This makes it an attractive algorithm for very large datasets.\n",
    "\n",
    "K-means is a greedy algorithm, i.e. it will only find a local minimum of the inertia. \n",
    "Therefore it is good practice to run the algorithm several times with different initialization.\n",
    "\n",
    "In `sklearn`, the [Kmeans implementation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) uses a *smart* intialization strategy (default), or a random initialization.\n",
    "\n",
    "The number of different initializations is controled with the `n_init` parameter (the default is 10 when initialization is random).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters = 3)\n",
    "\n",
    "km.fit( df_iris_pca ) ## k-means++ init, runs up to 300 iterations\n",
    "Kmean_plot( df_iris_pca , km.labels_ , km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25012f5",
   "metadata": {},
   "source": [
    "**Important remarks:**\n",
    "\n",
    " - KMeans is based on a concept of distances : your **data needs to be properly scaled**\n",
    " - KMeans creates, by construction, **globular clusters**\n",
    " - KMeans results depends on parameter **K**\n",
    "\n",
    "\n",
    "What values should we give to parameter K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d671e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots( 1,3 , figsize = (15,5) )\n",
    "for i,K in enumerate( [2,3,5] ):\n",
    "    km = KMeans(n_clusters = K)\n",
    "    km.fit( df_iris_pca ) ## k-means++ init, runs up to 300 iterations\n",
    "    Kmean_plot( df_iris_pca , km.labels_ , km.cluster_centers_, ax =ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7662def",
   "metadata": {},
   "source": [
    "We see that the number of clusters defines the k-means clustering outcome. It is difficult to say by looking at the t-SNE projections, whether 2 or 3 clusters better reveal underlying patterns of the data. \n",
    "\n",
    "There, we can compare the clustering proposed by different values of K with some of the clustering quality metrics provided by [scikit-learn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics). \n",
    " \n",
    "---\n",
    " \n",
    "In one case, we could **use known categories**, like the species for the iris example, when they exists AND when we think they should form meaningful clusters.\n",
    "\n",
    "An eample of such metric is the **Adjusted Rand Index** (ARI) which computes the similarity between two cluster assignments.\n",
    "\n",
    "The unadjusted version is computed with:\n",
    " * $s$ : the number of pairs of objects which have the same cluster in clustering 1 and the same in clustering 2 too\n",
    " * $d$ : the number of pairs of objects which have different clusters in clustering 1 and different clusters in clustering 2 too\n",
    " * $p$ : the total number of possible pairs of objects\n",
    "\n",
    "$$RI = \\frac{s+d}{p}$$\n",
    "\n",
    "The \"Adjusted\" version corresponds to a normalization that ensures that random clusterings give a value close to 0 and perfectly concordant clusterings a value of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARI example\n",
    "from sklearn.metrics import rand_score , adjusted_rand_score\n",
    "labels_true = [ 0, 0 , 1 , 1 , 1 , 0 , 0 ] \n",
    "labels_pred = [ 0, 1 , 0 , 1 , 1 , 0 , 0 ] \n",
    "\n",
    "##   possible pairs of objects : (0,1) (0,2) (1,2)  ...\n",
    "## in clustering 1             :   s     d     d    ...\n",
    "## in clustering 2             :   d     s     d    ...\n",
    "##                                             ^ this one is different in both clusterings \n",
    "\n",
    "print(\"RI: \", rand_score(labels_true, labels_pred))\n",
    "print(\"ARI:\", adjusted_rand_score(labels_true, labels_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc03793",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Another way, in the more common case where we do not know the categories in advances would be to use some metric which **computes how each cluster is cohesive and well separated from the others**.\n",
    "\n",
    "Here we will use the [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) metric: \n",
    "\n",
    "The **silhouette score** is **calculated for each point** using \n",
    "\n",
    "* the mean intra-cluster distance : $a$\n",
    "* the distance between a sample and the nearest cluster that the sample is not a part of : $b$\n",
    "\n",
    "\n",
    "The Silhouette score for a single point is $SC=\\frac{(b-a)}{max(a,b)}$\n",
    "\n",
    "The sklearn function `silhouette_score` (from `sklearn.metrics`) returns the mean Silhouette score over all samples for a given clustering result. To obtain the values for each sample, use `silhouette_samples`.\n",
    "\n",
    "$SC$ is always between -1 (bad clustering) and 1 (perfect clustering).\n",
    "\n",
    "![silhouette illustration](../images/silhouette.png)\n",
    "\n",
    "So the **silhouette score should be maximized.**\n",
    "\n",
    "> Warning : some score should be maximized and some other minimized. Be sure you know which one it should be before interpreting your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77fdc8",
   "metadata": {},
   "source": [
    "--- \n",
    "Let's try to apply this logic to the iris data-set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "ARIs = []\n",
    "silhouettes = []\n",
    "\n",
    "Ks = range( 2,10 )\n",
    "for i,K in enumerate( Ks ):\n",
    "    km = KMeans(n_clusters = K)\n",
    "    km.fit( df_iris_pca ) \n",
    "    \n",
    "    ## computing ARI and silhouette\n",
    "    ARIs.append(        adjusted_rand_score(iris['target'], km.labels_) )\n",
    "    silhouettes.append( silhouette_score( df_iris_pca , km.labels_ )    )\n",
    "    \n",
    "\n",
    "fig,ax = plt.subplots( 1,2 , figsize = (10,5) )\n",
    "ax[0].plot( Ks, ARIs )\n",
    "ax[0].set_ylabel('ARI')\n",
    "ax[0].set_xlabel('K')\n",
    "ax[0].grid(axis = 'x')\n",
    "ax[1].plot( Ks, silhouettes )\n",
    "ax[1].set_ylabel('silhouette')\n",
    "ax[1].set_xlabel('K')\n",
    "ax[1].grid(axis = 'x')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d9353",
   "metadata": {},
   "source": [
    "**Question:** What is the best K according to each metric ? Which one would you choose ?\n",
    "\n",
    "remember, the data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec73d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_iris_pca[:,0],y=X_iris_pca[:,1] )#,hue = iris['target'] , palette = ['#1b9e77','#d95f02','#7570b3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780de2f",
   "metadata": {},
   "source": [
    "### Micro-exercise: Kmeans\n",
    "\n",
    "Perform a kmean clustering on the extravert-introvert dataset. What is the best number of clusters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( dfpca_ie , x =  \"PC0\" , y = \"PC1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebab52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_02_ME1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc115a28",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## Hierarchical clustering <a class=\"anchor\" id=\"HC-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20374e01",
   "metadata": {},
   "source": [
    "Hierarchical clustering is an algorithms that iteratively merges data points or groups of data points. It starts with $N$ clusters, one cluster per data point. It first calculates the pairwise distances between all clusters. It then selects the two closests clusters and merges them into a new cluster. Next it recalculates the distances between the newly formed cluster and the remaining clusters. It repeats these steps until a specified number of clusters is reached or until all distances between clusters are larger than a specified threshold.\n",
    "\n",
    "![HC](../images/Hierachi.png)\n",
    "\n",
    "Since this type of algorithm requires pairwise distance calculations and does this up to $N$ times until all clusters are merged, the complexity of the algorithms is $N^3$. This is often too slow for large datasets. The algorithms can be accelarated by speeding up disnace calculations or initialize the clusters with a coarse k-means clustering.\n",
    "\n",
    "The main advantage of the algorithm is the visualization of the results as dendograms and the easy interpretability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd1266",
   "metadata": {},
   "source": [
    "AgglomerativeClustering proposes a number of options regarding how to compute the similarity between clusters: the `linkage` option:\n",
    "\n",
    "* `ward` minimizes the variance of the clusters being merged.\n",
    "* `average` uses the average of the distances of each observation of the two sets.\n",
    "* `complete` or maximum linkage uses the maximum distance between all observations of the two sets.\n",
    "* `single` uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "Take a moment to think on the implication these choice of linkage could have when clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4dc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the scipy [dendrogram](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html) function to plot the corresponding dendrogram.\n",
    "# adapter function from \n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "# to plot the dendrogram that will be explained below\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "def plot_dendrogram(model, label_colors = None , **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    R = dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "    ## coloring the labels\n",
    "    if not label_colors is None:\n",
    "        lbls = kwargs['ax'].get_xmajorticklabels()\n",
    "        if kwargs.get('orientation','bottom') in ['left','right']:\n",
    "            lbls = kwargs['ax'].get_ymajorticklabels()\n",
    "            for i,lbl in enumerate( lbls ):\n",
    "                color = label_colors[ R['leaves'][i] ]\n",
    "                lbl.set_color( color )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "## generating some random data\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
    "\n",
    "## you can try this instead:\n",
    "#X, y = make_blobs(n_samples=200, centers = 3 , cluster_std=3.0, random_state=42)\n",
    "\n",
    "\n",
    "# try different linkages:\n",
    "#    average : uses the average of the distances of each observation of the two sets.\n",
    "#    complete : or maximum linkage uses the maximum distance between all observations of the two sets.\n",
    "#    single : uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "# fig,ax = plt.subplots( 1 , 1 , figsize=(3, 3))\n",
    "# sns.scatterplot()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots( 2 , 3 , figsize=(10, 10))\n",
    "\n",
    "for i,link in enumerate(['complete' , 'average' , 'single']):\n",
    "    hclust = AgglomerativeClustering(n_clusters=2,compute_distances=True, linkage=link)\n",
    "    hclust.fit(X)\n",
    "    \n",
    "    plot_dendrogram( hclust , ax=ax[0,i])\n",
    "    ax[0,i].set_title(\"link = \"+link)\n",
    "    \n",
    "    sns.scatterplot( x = X[:,0] , y = X[:,1], hue = hclust.labels_ , ax=ax[1,i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83301d63",
   "metadata": {},
   "source": [
    "let's apply this on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e097e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hclust = AgglomerativeClustering(n_clusters=3 , compute_distances=True)\n",
    "hclust.fit(X_iris_pca)\n",
    "\n",
    "fig,ax = plt.subplots( 1,2 , figsize = (10,10) )\n",
    "sns.scatterplot(x=X_iris_pca[:,0],y=X_iris_pca[:,1] , hue = hclust.labels_.astype(str), s= 150,\n",
    "                style = iris.target_names[iris['target']], ax=ax[0])\n",
    "plot_dendrogram( hclust , \n",
    "                 color_threshold=10,\n",
    "                 labels= iris.target_names[iris['target']],  \n",
    "                 label_colors = np.array(['#1b9e77','#d95f02','#7570b3'])[ iris['target'] ],\n",
    "                 orientation='left',leaf_font_size=5,\n",
    "                 ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "link = 'ward'\n",
    "## we go from 2 to 32 clusters \n",
    "nr_clusters = np.arange(2,32)\n",
    "\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "for n in nr_clusters:\n",
    "    hclust = AgglomerativeClustering(n_clusters=n, linkage=link)\n",
    "    hclust.fit(X_ph_norm)\n",
    "    \n",
    "    scores.append(silhouette_score(X_ph_norm,hclust.labels_) )\n",
    "    labels.append(hclust.labels_)\n",
    "\n",
    "    \n",
    "best_idx = np.argmax( scores )\n",
    "best_labels = labels[best_idx]\n",
    "best_score = scores[best_idx]\n",
    "best_nr_clusters = nr_clusters[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe21d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## report and plot\n",
    "print(Counter(best_labels))\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,7))\n",
    "\n",
    "\n",
    "ax[0].plot(nr_clusters, scores, ls=\"-\", lw=2)\n",
    "ax[0].set_xlabel('Number of clusters')\n",
    "ax[0].set_ylabel('silhouette score')\n",
    "ax[0].set_title('AgglomerativeClustering',fontsize=10)\n",
    "\n",
    "xpca = PCA().fit_transform( X_ph_norm )\n",
    "\n",
    "sns.scatterplot( x = xpca[:,0],y = xpca[:,1],\n",
    "                hue=best_labels,\n",
    "                ax =ax[1])\n",
    "ax[1].set_xlabel('First Dimension')\n",
    "ax[1].set_ylabel('Second Dimension')\n",
    "ax[1].set_title('AgglomerativeClustering(n_clusters={0}, linkage=\"{1}\")'.format(best_nr_clusters,link),fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dendro = AgglomerativeClustering(n_clusters=None,distance_threshold=0, linkage='ward')\n",
    "dendro.fit(X_ph_norm)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "\n",
    "plot_dendrogram(dendro,\n",
    "                color_threshold=10,\n",
    "                orientation='left',leaf_font_size=10 , ax =ax)\n",
    "\n",
    "ax.set_xlabel(\"distance\")\n",
    "ax.axvline(10, color='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81bfba",
   "metadata": {},
   "source": [
    "The dendogram is the representation of the effect of your-cut off to define a cluster. For example if you decide that your threshold is going to be 15 : then you only have two clusters. If you change it to 11 you will have 4 clusters. If you put it to 0 : each point is a cluster!\n",
    "\n",
    "---\n",
    "\n",
    "Last but not least, hierarchical clustering is very common in **heatmaps**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f25e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ph_norm.std(axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_variance = X_ph_norm.std(axis=1)\n",
    "high_var = X_ph_norm.loc[ peptide_variance > peptide_variance.quantile(0.9) , :]\n",
    "\n",
    "\n",
    "sns.clustermap( high_var,z_score=1,\n",
    "               figsize=(15,20),\n",
    "               cmap='bwr', center=0, yticklabels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d0522",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## What is the best method for clustering ? <a id='best'></a>\n",
    "\n",
    "As you surely suspect by now, there is no perfect method. \n",
    "Each algorithm makes different assumptions about the structure of your data and will thus behave well or bad depending on howyour data is actually structured.\n",
    "\n",
    "Let's demonstrate by displaying how different algorithm perform on different dataset (example adapted from the [sklearn documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, { 'n_clusters': 2 }),\n",
    "    (noisy_moons, { 'n_clusters': 2 }),\n",
    "    (varied, { 'n_clusters': 3 }),\n",
    "    (aniso, { 'n_clusters': 3 }),\n",
    "    (blobs, { 'n_clusters': 3 }),\n",
    "    (no_structure, { 'n_clusters': 3 })]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 13))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.95, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "\n",
    "for i_dataset, (dataset, params) in enumerate(datasets):\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    clustering_algorithms = [( 'Kmeans' , KMeans(n_clusters=params['n_clusters']) ),\n",
    "                             ( 'HC - Ward' , AgglomerativeClustering(n_clusters=params['n_clusters'],\n",
    "                                                                             linkage='ward') ),\n",
    "                             ( 'HC - single' , AgglomerativeClustering(n_clusters=params['n_clusters'],\n",
    "                                                                             linkage='single') ),\n",
    "                             ( 'HC - complete' , AgglomerativeClustering(n_clusters=params['n_clusters'],\n",
    "                                                                             linkage='complete') )]\n",
    "    \n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        \n",
    "        algorithm.fit(X)\n",
    "        y_pred = algorithm.labels_.astype(int)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                           '#f781bf', '#a65628', '#984ea3',\n",
    "                           '#999999', '#e41a1c', '#dede00'])[ :int(max(y_pred) + 1) ],\n",
    "                                      \n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "        plot_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb156a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e668b87a",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "\n",
    "## Exercise: EDA and Clustering - single cell RNAseq data<a id=\"exo-kmeans-single\"></a>\n",
    "\n",
    "Data from: [Li H, Courtois ET, Sengupta D, Tan Y et al. Reference component analysis of single-cell transcriptomes elucidates cellular heterogeneity in human colorectal tumors. Nat Genet 2017 May;49(5):708-718. PMID: 28319088](https://pubmed.ncbi.nlm.nih.gov/28319088/)\n",
    "\n",
    "The data consists in normalized expression data (log10 RPKM) of 561 cells from 7 different human cell lines.\n",
    "\n",
    "For convenience, the cell lines have been associated to colors.\n",
    "\n",
    "In the data table:\n",
    "\n",
    " * columns are genes\n",
    " * rows are cells\n",
    " \n",
    "You can presume that the data has already been properly normalized, scaled, and imputed.\n",
    " \n",
    "Instructions:\n",
    "\n",
    " 1. perform a PCA and vizualize the results\n",
    " 2. try to cluster the cells using K-Means or Hierarchical clustering. \n",
    "     Try several parameter values and evaluate your clustering with a metric.\n",
    " 3. try the same thing, but use only the 50 first PCA components. Do you get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the data\n",
    "X_rpkm = pd.read_csv(\"../data/GSE81861_log10_RPKM.csv.zip\" , index_col=0)\n",
    "X_rpkm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rpkm.shape ## 561 cells, 39900 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932fdcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a Serie associating each row to a single column\n",
    "index_colors = pd.Series(X_rpkm.index).apply(lambda x : x.rpartition('_')[2])\n",
    "index_colors.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915cd747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbf23f64",
   "metadata": {},
   "source": [
    "Solutions:\n",
    "\n",
    "1. perform a PCA and vizualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed587322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-14 solutions/solution_01_GSE81861.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091e4fdb",
   "metadata": {},
   "source": [
    "fancier representation with 3D scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 15-22 solutions/solution_01_GSE81861.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a2bd1",
   "metadata": {},
   "source": [
    "2. try to cluster the cells using K-Means or Hierarchical clustering. \n",
    "\n",
    "first a single Kmean with K=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 23-37 solutions/solution_01_GSE81861.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633edd09",
   "metadata": {},
   "source": [
    "Try several parameter values and evaluate your clustering with a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f15e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 38-78 solutions/solution_01_GSE81861.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a01a6",
   "metadata": {},
   "source": [
    "plotting best clustering according to the Silhouette criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 79-83 solutions/solution_01_GSE81861.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec708e6",
   "metadata": {},
   "source": [
    " 3. try the same thing, but use only the 50 first PCA components. Do you get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 84- solutions/solution_01_GSE81861.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7156eb",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "\n",
    "## Exercise: EDA and Clustering - mammals sleep <a id=\"exo-kmeans-mammals\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7084095",
   "metadata": {},
   "source": [
    "Dataset from [Allison&Cicchetti 1976](https://science.sciencemag.org/content/194/4266/732)\n",
    "\n",
    "\n",
    "* species: Species of animal\n",
    "* bw : Body weight (kg)\n",
    "* brw : Brain weight (g)\n",
    "* sws : Slow wave (\"nondreaming\") sleep (hrs/day)\n",
    "* ps : Paradoxical (\"dreaming\") sleep (hrs/day)\n",
    "* ts : Total sleep (hrs/day) (sum of slow wave and paradoxical sleep)\n",
    "* mls : Maximum life span (years)\n",
    "* gt : Gestation time (days)\n",
    "* pi : Predation index (1-5), 1 = least likely to be preyed upon\n",
    "* sei : Sleep exposure index (1-5), 1 = least exposed (e.g. animal sleeps in a well-protected den), 5 = most exposed\n",
    "* odi : Overall danger index (1-5) based on the above two indices and other information, 1 = least danger (from other animals), 5 = most danger (from other animals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bea44",
   "metadata": {},
   "source": [
    " 1. handle the NAs. A mean imputation should work here (ie. replace NAs by their column average)\n",
    " 2. perform a PCA. Plot the PCA projected data as well as the weight of each column on the axes. What can you say ?\n",
    " 3. use t-SNE to get an embedding of the data in 2D and represent it.\n",
    "     **bonus :** plot the species names in the embedded space with `plt.text`\n",
    " 4. perform a Kmean, or hierarchical clustering on the PCA projected data. What is the best number of cluster according to the silhouette score?\n",
    " 5. plot the t-SNE projected data colored according to the cluster they belong to.\n",
    " \n",
    " \n",
    "**bonus**\n",
    " * try other clustering methods (eg, DBscan if you have done it with Kmeans before)\n",
    " * some variables could make more sense as a ratio, such as the brain weight / total body weight for instance. try to add some of these to the dataframe (or remove a couple of columns) and see how the results are affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569aed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mam = pd.read_csv('../data/mammalsleep.csv',sep=',', header=0 , index_col=0)\n",
    "\n",
    "## some of these column make more sense as ratio\n",
    "##df_mam[\"brain_ratio\"] = df_mam.brw / df_mam.bw\n",
    "##df_mam[\"ps_ratio\"] = df_mam.ps / df_mam.ts\n",
    "\n",
    "## dropping the now redundant columns\n",
    "## df_mam.drop( columns = [\"brw\",'ps','sws'] , inplace=True )\n",
    "\n",
    "df_mam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mam.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mam.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef3fec",
   "metadata": {},
   "source": [
    "Uncomment and execute to see the solution for each part of the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-5 solutions/solution_01_kmean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 7-42 solutions/solution_01_kmean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 44-60 solutions/solution_01_kmean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 62-81 solutions/solution_01_kmean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899425e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 83- solutions/solution_01_kmean.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e99a5a",
   "metadata": {},
   "source": [
    "# Appendix <a class=\"anchor\" id=\"appendix-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67dbdbb",
   "metadata": {},
   "source": [
    "## DBSCAN <a class=\"anchor\" id=\"DBSCAN-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da9e47d",
   "metadata": {},
   "source": [
    "DBSCAN - **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise [Ester et al., Proc KDD, 1996](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.9220) finds core points of high density and expands clusters from them. It works well for data which contains **clusters of similar density**. \n",
    "\n",
    "The algorithm starts by defining core points (points that are densely packed : at least `min_samples` points within a distance `eps` from each other). Then it expands a seed cluster $C_k$ by adding a point $p_j$ to the cluster if there a point $p_{l} \\in C_k$ with $dist(p_j-p_{l})<\\epsilon$. Points that cannot be linked to a seed cluster are marked as outliers.\n",
    "\n",
    "![db](../images/db.png)\n",
    "\n",
    "\n",
    "DBSCAN The average run time complexity of DBSCAN is $O(nlogn)$ (worst case $O(n^2)$), which makes attractive for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040df3b5",
   "metadata": {},
   "source": [
    "First, we need to have an idea in which range $\\epsilon$ will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "D = pd.DataFrame(pairwise_distances(X_im_pca, metric='euclidean'))\n",
    "dists = D[D>0].min(axis=1) # get nearest neighbor distances\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.histplot(dists, color=\"g\", binwidth=0.25)\n",
    "plt.xlabel(\"Distances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79de17",
   "metadata": {},
   "source": [
    "So it looks like our epsilon would be somewhere between 0.25 and 2.\n",
    "\n",
    "We will make our minimum number of samples vary from 5 to 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6083d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import cluster\n",
    "\n",
    "eps = np.arange(0.10, 3, 0.10)\n",
    "min_samples = np.arange(3,25,1)\n",
    "\n",
    "## keep the scores in a 2D matrix, for each combination of eps and min_sample\n",
    "scores = np.zeros( (len(eps) , len(min_samples)) )\n",
    "N_assigned = np.zeros( (len(eps) , len(min_samples)) )\n",
    "\n",
    "for i, e in enumerate( eps ):\n",
    "    best_eps_score = -1\n",
    "    for j, minS in enumerate( min_samples ):\n",
    "        dbscan = cluster.DBSCAN(eps=e, min_samples=minS)\n",
    "        dbscan.fit(X_im_pca)\n",
    "        \n",
    "        ## DBSCAN will detect some \"outliers\", which have a label of -1. We exclude them from the matric computation\n",
    "        idx = dbscan.labels_ >= 0\n",
    "        nr_clusters = len(np.unique(dbscan.labels_[idx]))\n",
    "        \n",
    "        N_assigned[i,j] = idx.sum()\n",
    "        scores[i,j] = silhouette_score(X_im_pca[idx],dbscan.labels_[idx]) if nr_clusters > 1 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4171766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we grab the combination with the highest value\n",
    "best_idx = np.unravel_index(scores.argmax(), scores.shape)\n",
    "best_eps = eps[best_idx[0]]\n",
    "best_min_samples = min_samples[best_idx[1]]\n",
    "\n",
    "# and we perform the clustering with the best values:\n",
    "dbscan = cluster.DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "best_labels = dbscan.fit_predict(X_im_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(15,7))\n",
    "\n",
    "sns.heatmap(scores,\n",
    "            xticklabels=min_samples, \n",
    "            yticklabels=[\"{:.2f}\".format(e) for e in eps],\n",
    "            cmap='coolwarm' , ax = axes[0])\n",
    "axes[0].set_ylabel('epsilon')\n",
    "axes[0].set_xlabel('min samples')\n",
    "axes[0].set_title('DBSCAN clustering - Silhouette score',fontsize=10)\n",
    "\n",
    "\n",
    "sns.scatterplot(x=X_im_tsne.embedding_[:,0],y=X_im_tsne.embedding_[:,1],hue=best_labels.astype(str), ax=axes[1])\n",
    "axes[1].set_xlabel('First Dimension')\n",
    "axes[1].set_ylabel('Second Dimension')\n",
    "axes[1].set_title('DBSCAN(eps={0:.2f}, min_samples={1})'.format(best_eps,best_min_samples),fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3df4af",
   "metadata": {},
   "source": [
    "As you can see the solution with the highest silhouette score leaves a lot of points unclassified. \n",
    "\n",
    "This is something one has to be careful about with methods such as DBSCAN which have the option of leaving some points out, and can potentially be used to determine that this algorithm is not the most adapted to the nature of the data.\n",
    "\n",
    "You can for example look at the number of points assigned to a cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19892501",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1,figsize=(7,7))\n",
    "sns.heatmap(N_assigned,\n",
    "            xticklabels=min_samples, \n",
    "            yticklabels=[\"{:.2f}\".format(e) for e in eps],\n",
    "            cmap='coolwarm' , ax=axes )\n",
    "axes.set_xlabel('min samples')\n",
    "axes.set_ylabel('epsilon')\n",
    "axes.set_title('DBSCAN - number of points assigned to a cluster'.format(best_eps,best_min_samples),fontsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d4645",
   "metadata": {},
   "source": [
    "**Question:** what other aspects could we look at to evaluate a clustering created by DBSCAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e9c66",
   "metadata": {},
   "source": [
    "## Gaussian mixtures <a class=\"anchor\" id=\"gm-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed5aea",
   "metadata": {},
   "source": [
    "In Gaussian mixture modeling [Fraley & Raftery, The Computer Journal, 1998](https://academic.oup.com/comjnl/article-abstract/41/8/578/360856) we represent the data $\\pmb{X}$ as a mixture model normal probability distributions:\n",
    "\n",
    "$p(\\pmb{x}) = \\sum_{i=1}^{K}\\pi_iN(\\pmb{x}|\\pmb{\\mu_i},\\pmb{\\Sigma}_i)$, \n",
    "\n",
    "where $N(\\pmb{x}|\\pmb{\\mu},\\pmb{\\Sigma})$ is a multidimensional Gaussian distribution with mean $\\pmb{\\mu}$ and covariance matrix $\\pmb{\\Sigma}$. The method uses the expectation-maximization algorithm to find the mixture components $\\pmb{\\pi_i}$, $\\pmb{\\mu_i}$ and covariance matrix $\\pmb{\\Sigma}_i$.\n",
    "\n",
    "The sklearn [GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) class implements the algorithm. It allows you to specify the number of clusters ('n_components'), intitial values for the parameters and constrains on the covariance matrices $\\pmb{\\Sigma}_i$. \n",
    "\n",
    "A Bayesian version [BayesianGaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture) uses priors for the mixture parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "nr_clusters = np.arange(9)+2\n",
    "\n",
    "aic = []\n",
    "best_score = np.inf\n",
    "best_nr_clusters = 0\n",
    "for n in nr_clusters:\n",
    "    gmm = mixture.GaussianMixture(n)\n",
    "    gmm.fit(X_im_scaled)\n",
    "    score = gmm.bic(X_im_scaled)\n",
    "    aic.append(score)\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_nr_clusters = n\n",
    "        best_gmm = gmm\n",
    "        \n",
    "best_labels = best_gmm.predict(X_im_scaled)\n",
    "\n",
    "bgmm = mixture.BayesianGaussianMixture(n_components=10,\n",
    "                                       covariance_type='full',\n",
    "                                       weight_concentration_prior=0.01, max_iter=500)\n",
    "bgmm.fit(X_im_scaled)\n",
    "best_labels_bayes = bgmm.predict(X_im_scaled)\n",
    "print(np.sort(bgmm.weights_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc56ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "\n",
    "ax[0].plot(nr_clusters, aic, ls=\"-\", lw=2)\n",
    "ax[0].set_xlabel('Number of clusters')\n",
    "ax[0].set_ylabel('AIC  score')\n",
    "ax[0].set_title('GaussianMixture',fontsize=10)\n",
    "\n",
    "\n",
    "ax[1].scatter(X_im_tsne.embedding_[:,0],X_im_tsne.embedding_[:,1],c=best_labels,s=50,cmap='plasma')\n",
    "ax[1].set_xlabel('First Dimension')\n",
    "ax[1].set_ylabel('Second Dimension')\n",
    "ax[1].set_title('GaussianMixture(n_components={0})'.format(best_nr_clusters),fontsize=10)\n",
    "\n",
    "\n",
    "ax[2].scatter(X_im_tsne.embedding_[:,0],X_im_tsne.embedding_[:,1],c=best_labels_bayes,s=50,cmap='plasma')\n",
    "ax[2].set_xlabel('First Dimension')\n",
    "ax[2].set_ylabel('Second Dimension')\n",
    "ax[2].set_title('BayesianGaussianMixture(n_components={0})'.format(10),fontsize=10)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa13089",
   "metadata": {},
   "source": [
    "# Additionnal exercise : Single cell dataset - preprocessing <a class=\"anchor\" id=\"exo-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33064f",
   "metadata": {},
   "source": [
    "In this exercice we will be playing with a single cell RNAseq dataset from [Li et al. 2017](https://www.nature.com/articles/ng.3818#Sec28).\n",
    "\n",
    "In their article they evaluate several advanced clustering techniques specifically developped for RNAseq data.\n",
    "\n",
    "In this exercise, we will be using the techniques we saw in this chapter, and also exploring the effect of normalization.\n",
    "\n",
    "> Warning: this dataset is large, and present a no so easy clustering problem. Do not hesitate to take your time and test some of your code on a smaller subset of the data.\n",
    "\n",
    "\n",
    "Let's start by reading the Count matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/GSE81861_Cell_Line_COUNT.csv', index_col=0)\n",
    "df = df.loc[ df.sum(axis=1) != 0 , :] # we remove transcripts whose count is 0 in all cell\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca23286",
   "metadata": {},
   "source": [
    "In this matrix:\n",
    " - rows are transcripts\n",
    " - columns are cell\n",
    " - values correspond to the number of reads that were assigned to a given transcript in a given cell\n",
    " \n",
    "Here, you can see that our samples (cells) are in columns, and our variables (transcripts) are in rows, which is the convention for RNAseq data, but not for `sklearn` machine learning routines, so you need to transpose this data:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6122a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc53b68",
   "metadata": {},
   "source": [
    "### tasks\n",
    "\n",
    "We recommend you read all tasks and notes before starting to tackle this exercise.\n",
    "\n",
    "#### A. normalization \n",
    "\n",
    "We will test the impact of normalization techniques on clustering. For this, create 2 versions of the dataset:\n",
    "1. log10-transform the data with a pseudo-count of 1, then use the StandardScaler to scale the data\n",
    "2. use the method of the paper : computing [RPKM](https://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/) and filtering, following these steps:\n",
    "    1. Count up the total reads in a sample and divide that number by 1,000,000 – this is our “per million” scaling factor.\n",
    "    2. Divide the read counts by the “per million” scaling factor. This normalizes for sequencing depth, giving you reads per million (RPM)\n",
    "    3. Divide the RPM values by the length of the gene, in kilobases, as found in the `../data/GSE81861_transcript_length.csv` file. This gives you RPKM.\n",
    "    4. Keep only transcripts with a RPKM >= $10^{-3}$ in at least 2 cells\n",
    "    5. log10-transform the data with a pseudo-count of $10^{-3}$\n",
    "    \n",
    "> NB: log-transforming X with a pseudocount of Y means: log(Y+X), this is done to avoid mathermatical errors whenthere are 0s in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d153c8",
   "metadata": {},
   "source": [
    "#### B. dimensionality reduction for visualization\n",
    "\n",
    "3. Compute a PCA and a t-sne representation for each of the normalized dataset. Find appropriate values of the perplexity parameter for both (somewhere between 1 and 100).\n",
    "\n",
    "#### C. hierarchical clustering \n",
    "\n",
    "For both normalized datasets:\n",
    "4. try hierarchical clustering with \"ward linkage, euclidean distance\" and \"average linkage, cosine distance\", with  a number of clusters between 2 and 20, and measure the silhouette score each time.\n",
    "\n",
    "> **Warning:** adapt the silhouette score to the metric distance you are currently using.\n",
    "\n",
    "> **Warning2:** Hierarchical clustering on this data takes 2 to 5 seconds. With 2 normalizations options, 2 linkage/distance combinations, and 19 cluster numbers to evaluate, so 2*2*19=76 parameter combinations, running clustering for all parameter combinations can take around 3-6 minutes, so be sure to test your code on a smaller number of combinations firsts\n",
    "\n",
    "5. What is the and parameter combinations (normalization technique, linkage, distance, and number of clusters) which gives the highest silhouette coefficient? \n",
    "6. Visualize the clusters given by this parameter combination on the relevant PCA and t-sne projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d16e5",
   "metadata": {},
   "source": [
    "#### D. hierarchical clustering with an objective\n",
    "\n",
    "In this experiment the cell actually belong to known cell types which are encoded in the cell names.\n",
    "\n",
    "You can get the cell categories with the following code:\n",
    "```{python}\n",
    "cell_categories = pd.Series(df.columns).apply( lambda x : x.partition(\"_\")[2].rpartition('_')[0].strip('_') )\n",
    "```\n",
    "\n",
    "They correspond to :\n",
    " - A549 : lung carcinoma\n",
    " - GM12878 : lymphoblastoid\n",
    " - H1437 : lung adenocarcinoma\n",
    " - IMR90 : fetal lung fibroblast\n",
    " - H1 : human embryonic stem cell\n",
    " - K562 : myelogenous leukemia\n",
    "\n",
    "With some cell types presenting thecnical replicates (H1 for example).\n",
    "\n",
    "Ideally, these cell types would correspond to clusters in the data, and the goal of the original study is to find which clustering method is able to find clusters which correspond as much as possible to the cell categories.\n",
    "\n",
    "We will be doing something along these lines.\n",
    "\n",
    "First, let's use what we already have:\n",
    "\n",
    "7. compare the proposed clustering with the known categories of cells\n",
    "8. looking at the silhouette score trajectories, and considering the known categories of cell, do you see sets of clustering parameters which may not have the best silhouette score but may be closer to the cell categories? check out that hypothesis.\n",
    "\n",
    "To approach this problem in a more principled way, we will use a score which compare two clustering : the adjusted rand index, computed using the [adjusted_rand_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) sklearn function.\n",
    "\n",
    "\n",
    "\n",
    "9. Compute the adjusted random index to choose the normalization method and clustering parameters which resemble the cell categories the best\n",
    "10. Represent the best clustering according to the adjusted rand index\n",
    "11. What do yo think about this shift in perspective?\n",
    "\n",
    "> If you have read this until the end before performing all the computations, then you can see that during step 4 you could also record the adjusted rand index alongside the silhouette score, so that you don't have to perform the long computations again for question 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_introml2024)",
   "language": "python",
   "name": "conda_introml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
