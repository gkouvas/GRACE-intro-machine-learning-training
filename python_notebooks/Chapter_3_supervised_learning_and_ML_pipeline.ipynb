{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 5, 5\n",
    "plt.rc(\"font\", size=10)\n",
    "\n",
    "\n",
    "plt.rc('xtick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('xtick.major', size=8, pad=12)\n",
    "plt.rc('xtick.minor', size=8, pad=12)\n",
    "\n",
    "plt.rc('ytick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('ytick.major', size=8, pad=12)\n",
    "plt.rc('ytick.minor', size=8, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66fd46",
   "metadata": {},
   "source": [
    "In this chapter we will use a simple regression case to introduce some **core concepts of Supervised Learning and  Machine Learning**. \n",
    "\n",
    "\n",
    "\n",
    "![image showing the difference between classification (left) and regression (right). On the left, set of red and blue points are separated by a black line which represent our model. In contrast, the model's line follow along the patterns of the points cloud](../images/class_reg.png)\n",
    "\n",
    "Supervised Learning considers tasks where we want to predict a predefined target. If the target is categorical we talk about a **classification task**. If the target is continuous we talk about **regression task**.\n",
    "\n",
    "\n",
    "Regression and classification use different models and optimize different metric,\n",
    "but in Machine Learning they also share many common practices and concerns.\n",
    "\n",
    "\n",
    "The machine learning paradigm emphasizes the importance of building a model that will be good \n",
    "**not only on the data it has been trained on, but also on new data**.\n",
    "In other words, we want a model that can be **generalized to new data**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2db98",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To introduce this as well as some of the core concepts of Machine Learning, we will illustrate with a relatively *simple* case of a linear modelling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b214fda",
   "metadata": {},
   "source": [
    "# Table Of Content: <a id=\"toc\"></a>\n",
    "\n",
    "\n",
    "* [**motivating example**](#motivation)\n",
    "* [approach 1: a simple linear regression](#linear-1)\n",
    "* [approach 2: adding regularization and validation set](#linear-2)\n",
    "* [approach 3 : k-fold cross-validation](#linear-3)\n",
    "* [approach 4 : a \"classical\" ML pipeline](#linear-4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ed0ac",
   "metadata": {},
   "source": [
    "# motivating example <a id=\"motivation\"></a>\n",
    "\n",
    "[Acharjee et al.2016](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1043-4) propose several -omic dataset which they used to predict and gain knowledge on various phenotypic traits in potatos.\n",
    "\n",
    "Here, we will concentrate on their transcriptomics dataset and the phenotypic trait of the potato coloration.\n",
    "\n",
    "We have pre-selected and normalized the 200 most promising genes (out of ~15 000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_metadata = \"../data/potato_data.phenotypic.csv\"\n",
    "file_data = \"../data/potato_data.transcriptomic.top200norm.csv\"\n",
    "\n",
    "df = pd.read_csv( file_metadata , index_col=0 )\n",
    "dfTT = pd.read_csv( file_data , index_col=0)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214de97",
   "metadata": {},
   "source": [
    "For the sake of our story, we will imagine that out of the 86 potatos in the data, we have only 73 at the time of our experiment.\n",
    "\n",
    "We put aside the rest for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = df.index[:73]\n",
    "i2 = df.index[73:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfTT.loc[i1 , :]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[i1 , \"Flesh Colour\"]\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2196f7c",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## approach 1: a simple linear regression <a class=\"anchor\" id=\"linear-1\"></a>\n",
    "\n",
    "\n",
    "We will be modelling the potato flesh color using a simple [linear model](https://en.wikipedia.org/wiki/Linear_model),\n",
    "where we would write the equation of the flesh color something like:\n",
    "\n",
    "\n",
    "$$flesh color = \\beta_1 * E1 + \\beta_2 * E2 + ... + \\beta_n * En $$\n",
    "\n",
    "Where $Ex$ is the expression level of gene $x$, and $\\beta_x$ is the associated coefficient (which describes how much the expression of gene $x$ contributes to the flesh color prediction).\n",
    "\n",
    "\n",
    "\n",
    "> It is very likely that you have already seen and manipulated linear models previously, and we are not going to develop much further here. But in case this is the first time you meet them, or you just want a refresher, here is a [jupyter notebook on linear model](https://github.com/sib-swiss/statistics-and-machine-learning-training/blob/main/01_regression_Least_Square.ipynb), from one of our other courses ;-) \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Let's fit a simple linear model with our gene expression values, and see what happens\n",
    "\n",
    "\n",
    "For this, we will use the `sklearn` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd56004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we import elements from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score , mean_squared_error\n",
    "\n",
    "\n",
    "# create the regression object\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# fit it with our data\n",
    "lin_reg.fit(X,y)\n",
    "\n",
    "# predict\n",
    "y_pred = lin_reg.predict( X )\n",
    "\n",
    "# evaluate the prediction\n",
    "print(f\"R-squared score: { r2_score( y , y_pred ) :.2f}\")\n",
    "print(f\"mean squared error: { mean_squared_error( y , y_pred ) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bc1c0",
   "metadata": {},
   "source": [
    "\n",
    "**Wow!!** this is a perfect fit.\n",
    "\n",
    "But if you know anything about biology, or data analysis, then you likely suspect something wrong is happening.\n",
    "\n",
    "\n",
    "Indeed, at the moment, our claim is that our model can predict flesh color perfectly (RMSE=0.0) from the normalized expression of these 200 genes.\n",
    "\n",
    "But, say we now have some colleagues who come to us with some new potato data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we use the leftover data points:\n",
    "Xnew = dfTT.loc[i2 , :]\n",
    "ynew = df.loc[i2 , \"Flesh Colour\"]\n",
    "\n",
    "## apply the model on the new data\n",
    "ynew_pred = lin_reg.predict( Xnew )\n",
    "\n",
    "# evaluate the prediction\n",
    "print(f\"new data R-squared score: { r2_score( ynew , ynew_pred ) :.2f}\")\n",
    "print(f\"new data mean squared error: { mean_squared_error( ynew , ynew_pred ) :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( y , y_pred , label = 'training data' )\n",
    "plt.scatter( ynew , ynew_pred , label = 'new data' )\n",
    "plt.xlabel('observed values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc3907",
   "metadata": {},
   "source": [
    "As expected, the performance on the new data is not as good as with the data we used to train the model.\n",
    "\n",
    "We have **overfitted** the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6050578",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here we could still use the model that we have created, \n",
    "but we would agree that reporting the perfect performance we had with our training data would be misleading.\n",
    "\n",
    "To honestly report the performance of our model, we measure it on a **set of data that has not been used at all to train it: the *test set*.**\n",
    "\n",
    "\n",
    "\n",
    "To that end, we typically begin by dividing our data into :\n",
    "\n",
    " * **train** set : find the best model\n",
    " * **test** set  : give an honest evaluation of how the model perform on completely new data.\n",
    "\n",
    "![train_test](../images/train_test.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d451906",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's rename the python objects to reflect this concept:\n",
    "\n",
    "X_test = Xnew\n",
    "y_test = ynew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1fa00",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## approach 2: adding regularization and validation set <a class=\"anchor\" id=\"linear-2\"></a>\n",
    "\n",
    "In the case of a Least Square fit, the function you are minimizing looks like:\n",
    "\n",
    "$\\sum_i (y_i-y\\_pred_i))^2$\n",
    "\n",
    ", so the sum of squared difference between the observation and the predictions of your model.\n",
    "\n",
    "\n",
    "And remember that our model is written in the following way\n",
    "\n",
    "$$y\\_pred_i = \\beta_1 * X1_{i} + \\beta_2 * X2_{i} + ... + \\beta_n * Xn_{i} $$\n",
    "\n",
    "---\n",
    "\n",
    "**Regularization** is a way to reduce overfitting, and in the case of the linear model\n",
    "we do so by adding to this function a **penalization term which depends on coefficient weights** (*ie*, the $\\beta_x$).\n",
    "\n",
    "In brief, the stronger the coefficient, the higher the penalization. So only coefficients which bring more fit than penalization will be kept.\n",
    "\n",
    "\n",
    "> Note : we report here the formulas used in `scikit-learn` functions. Other libraries may have a different parameterization, but the concepts stay the same\n",
    "\n",
    "\n",
    "$\\frac{1}{2n}\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}|\\beta_{j}|$ , **l1 regularization** (Lasso) $\\alpha$ being the weight that you put on that regularization \n",
    "\n",
    "\n",
    "$\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}\\beta_{j}^{2}$ , **l2 regularization** (Ridge) \n",
    "\n",
    "\n",
    "$\\frac{1}{2n}\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}(\\rho|\\beta_{j}|+\\frac{(1-\\rho)}{2}\\beta_{j}^{2})$ , **elasticnet**\n",
    "\n",
    "\n",
    "For a deeper understanding of those notions, you may look at :\n",
    "\n",
    " * [datacamp tutorial](https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net)\n",
    "\n",
    " * [towardsdatascience tutorial](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)\n",
    "\n",
    "\n",
    "\n",
    "> NB: Regularization generalize to maximum likelihood contexts as well)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8b128",
   "metadata": {},
   "source": [
    "You can see there is a **parameter $\\alpha$: the regularization strength**\n",
    "\n",
    "A small $\\alpha$ will give you almost the same model as before.\n",
    "\n",
    "A large $\\alpha$ will give you a model where only a few coefficient are allowed to have a significant impact on the prediction.\n",
    "\n",
    "\n",
    "Let's try that on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67376e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDRegressor     \n",
    "# we import the Stochastic Gradient Descent Regressor\n",
    "# which implements the algorithm to find the best coefficient \n",
    "# when there is a regularization penalty\n",
    "\n",
    "logalphas = []\n",
    "\n",
    "coef_dict = {'name' : [],\n",
    "             'coefficient' : [],\n",
    "             'log-alpha' : []}\n",
    "r2 = []\n",
    "\n",
    "## we try different values of alpha. \n",
    "for alpha in np.logspace(-2,2,50):\n",
    "\n",
    "    reg = SGDRegressor( penalty='l1' , alpha = alpha ) # create the model \n",
    "    reg.fit( X , y )                                   # fit the model \n",
    "    \n",
    "    logalphas.append(np.log10(alpha))\n",
    "    r2.append( r2_score( y , reg.predict(X) ) )       # compute R2 score\n",
    "    \n",
    "    \n",
    "    ## I keep the value of the coefficients to show what the regularization does\n",
    "    coef_dict['name'] += list( X.columns )\n",
    "    coef_dict['coefficient'] += list( reg.coef_ )\n",
    "    coef_dict['log-alpha'] += [np.log10(alpha)]* len(X.columns )\n",
    "\n",
    "coef_df = pd.DataFrame(coef_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4964d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (14,7))\n",
    "\n",
    "ax[0].plot(logalphas , r2)\n",
    "ax[0].set_xlabel(\"log10( alpha )\")\n",
    "ax[0].set_ylabel(\"R2\")\n",
    "\n",
    "sns.lineplot( x = 'log-alpha' , y='coefficient' , hue = 'name' , data= coef_df , ax = ax[1] ,legend = False)\n",
    "\n",
    "fig.suptitle(\"regression of potato data with an L1 regularization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc72e4",
   "metadata": {},
   "source": [
    "On the left subplot you see how the R-squared score changes with $log10(\\alpha)$.\n",
    "\n",
    "\n",
    " * **High $R^2$ at low $\\alpha$**: at lower values of $log10(\\alpha)$ (left side of the plot), the R-squared score is high, indicating that the model fits the data well. This is because the regularization is weak, allowing more features to contribute to the model.\n",
    " * **Sharp Drop in $R^2$**: as $log10(\\alpha)$ increases, the R-squared score drops sharply. This indicates that stronger regularization (higher alpha values) leads to a simpler model with fewer features, which may not capture the dataâ€™s variance as well.\n",
    " * **$R^2 = 0$ at high $\\alpha$**: at higher values of $log10(\\alpha)$ (right side of the plot), the R-squared score is 0, indicating that regularization is so strong that no coefficient are contributing to the predictions (*ie*, their values are all at 0).\n",
    "\n",
    "\n",
    "The right subplot confirms this interpretation as it shows the coefficients values for each of level of $\\alpha$\n",
    "\n",
    " * Each line represents the path of a coefficient for a specific feature (here a gene) as $\\alpha$ varies.\n",
    " * As log-alpha increases, many **coefficients shrink towards zero**. This is the effect of L1 regularization, which penalizes the absolute size of the coefficients, leading to sparse models where many coefficients become exactly zero.\n",
    " * Features with coefficients that shrink to zero first are less important for the model. Features whose **coefficients remain non-zero for higher values of log-alpha are more important for the model**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7466917",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Micro-exercise:** adapt the code above to generate this plot with an l2 penalty. How do you interpret the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_mini1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b50af1",
   "metadata": {},
   "source": [
    "This is great, but how do we choose which level of regularization we want ?\n",
    "\n",
    "It is a general rule that **as you decrease $\\alpha$, the $R^2$ on the data used for the fit increase**, i.e. you risk overfitting.\n",
    "\n",
    "Consequently, we cannot choose the value of $\\alpha$ parameter from the data used to fit alone; we call such a parameter an **hyper-parameter**.\n",
    "\n",
    "**Question:** what are other hyper-parameters at this point?\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "In order to find the optimal value of an hyper-parameter, we can separate our training data into:\n",
    " * a **train set** : used to fit the model\n",
    " * a **validation set** : used to evaluate how our model perform on new data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## adding a validation set\n",
    "\n",
    "# we will use 60 points to train the model\n",
    "# and we will use the rest to evaluate the model \n",
    "I = list( range(X.shape[0]))\n",
    "np.random.shuffle( I ) \n",
    "\n",
    "I_train = I[:60]\n",
    "I_valid = I[60:]\n",
    "\n",
    "X_train = X.iloc[ I_train , : ] \n",
    "y_train = y.iloc[ I_train ]\n",
    "\n",
    "# we will use the rest to evaluate the model\n",
    "X_valid = X.iloc[ I_valid , : ] \n",
    "y_valid = y.iloc[ I_valid ]\n",
    "\n",
    "\n",
    "logalphas = []\n",
    "\n",
    "r2_train = []\n",
    "r2_valid = []\n",
    "\n",
    "for alpha in np.logspace(-3,2,200):\n",
    "\n",
    "    reg = SGDRegressor( penalty='l1' , alpha = alpha  )\n",
    "    reg.fit( X_train , y_train )\n",
    "    \n",
    "    logalphas.append(np.log10(alpha))\n",
    "    r2_train.append( r2_score( y_train , reg.predict(X_train) ) )\n",
    "    r2_valid.append( r2_score( y_valid , reg.predict(X_valid) ) )\n",
    "    \n",
    "## plotting and reporting \n",
    "bestI = np.argmax(r2_valid)\n",
    "bestLogAlpha = logalphas[bestI]\n",
    "bestR2_valid = r2_valid[bestI]\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (10,5))\n",
    "fig.suptitle(\"best alpha : {:.2f} - validation R2 : {:.2f}\".format(10**bestLogAlpha , bestR2_valid))\n",
    "ax.plot( logalphas, r2_train , label='train set' )\n",
    "ax.plot( logalphas, r2_valid , label='validation set' )\n",
    "ax.scatter( [bestLogAlpha] , [bestR2_valid]  , c='red')\n",
    "ax.set_xlabel(\"log10( alpha )\")\n",
    "ax.set_ylabel(\"R2\")\n",
    "ax.set_ylim(-0.1,1.1)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64beb570",
   "metadata": {},
   "source": [
    "So now, with the help of a validation set, we can clearly see the phases :\n",
    " * **underfitting** : for high $\\alpha$, the performance is low for both the train and the validation set\n",
    " * **overfitting** : for low $\\alpha$, the performance is high for the train set, and low for the validation set\n",
    " \n",
    "We want the equilibrium point between the two where performance is ideal for the validation set.\n",
    "\n",
    "**Problem :** if you run the code above several time, you will see that the optimal point varies due to the random assignation to train or validation set. \n",
    "\n",
    "There exists a myriad of possible strategies to deal with that problem, such as repeating the above many times and taking the average of the results for instance.\n",
    "\n",
    "> Note also that this problem gets less important as the validation set size increases.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Anyhow, on top of our earlier regression model, we have added :\n",
    "\n",
    " * an **hyper-parameter** : $\\alpha$, the strength of the regularization term\n",
    " * a **validation strategy** for our model in order to avoid overfitting\n",
    "\n",
    "<br>\n",
    "\n",
    "That's it, we are now in the world of Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5202e7f",
   "metadata": {},
   "source": [
    "But before we go any further, let's see how this modified model performs on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg = SGDRegressor( penalty='l1' , alpha = 10**bestLogAlpha  )\n",
    "reg.fit( X , y )\n",
    "\n",
    "y_pred = reg.predict( X )\n",
    "print(f\"train data R-squared score: { r2_score( y , y_pred ) :.2f}\")\n",
    "print(f\"train data mean squared error: { mean_squared_error(  y , y_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "y_test_pred = reg.predict( X_test )\n",
    "\n",
    "print(f\" test data R-squared score: { r2_score( y_test , y_test_pred ) :.2f}\")\n",
    "print(f\" test data mean squared error: { mean_squared_error(  y_test , y_test_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "plt.scatter( y , y_pred , label = 'training data' )\n",
    "plt.scatter( y_test , y_test_pred , label = 'new data' )\n",
    "plt.xlabel('observed values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e10365",
   "metadata": {},
   "source": [
    "Two things to observe:\n",
    " * we still see better performance on the train data than on the test data (generally always the case)\n",
    " * the performance on the test set has improved: our model is less overfit and more generalizable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b94b7",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "##  approach 3 : k-fold cross-validation <a class=\"anchor\" id=\"linear-3\"></a>\n",
    "\n",
    "In the previous approach, we have split our training data into a train set and a validation set.\n",
    "\n",
    "This approach works well if you have enough data for your validation set to be representative.\n",
    "\n",
    "Often, we unfortunately do not have enough data for this.\n",
    "\n",
    "Indeed, we have seen that if we run the code above several time, we see that the optimal point varies due to the random assignation to train or validation set. \n",
    "\n",
    "\n",
    "**K-fold cross validation** is one of the most common strategy to try to mitigate this randomness with a limited amount of data.\n",
    "\n",
    "![k-fold validation](../images/kfold.png)\n",
    "\n",
    "In k-fold cross-validation\n",
    "\n",
    "1. the data is split in $k$ subparts, called fold.\n",
    "2. $k$ models are trained for each hyper-parameter values combination: each time a different fold for validation (and the remaining $k-1$ folds for training).\n",
    "3. the average performance is computed across all fold : this is the **cross-validated performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575cf570",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "We are going to do a simple k-fold manually once, to explore a bit how it works, but in practice you will discover that it is mostly automatized with some of scikit-learn's recipes and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b06d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kfold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5 , shuffle=True , random_state=734)\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={valid_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ba149",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logalphas = np.linspace(-2,1,200)\n",
    "\n",
    "kf = KFold(n_splits=5 , shuffle=True , random_state=6581) ## try changing the random state\n",
    "\n",
    "fold_r2s = [ [] for i in range(kf.n_splits) ] ## for each fold\n",
    "cross_validated_r2 = [] # average across folds\n",
    "\n",
    "for j,alpha in enumerate( 10**logalphas ) :                       # hyper-parameter loop\n",
    "\n",
    "    cross_validated_r2.append(0)\n",
    "    \n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(X)):  # k-fold loop\n",
    "\n",
    "        ## split train and validation sets\n",
    "        X_train = X.iloc[ train_index , : ]\n",
    "        X_valid = X.iloc[ valid_index , : ]\n",
    "\n",
    "        y_train = y.iloc[ train_index ]\n",
    "        y_valid = y.iloc[ valid_index ]\n",
    "\n",
    "        ## fit model for that fold\n",
    "        reg = SGDRegressor( penalty='l1' , alpha = alpha  )\n",
    "        reg.fit( X_train , y_train )\n",
    "\n",
    "        ## evaluate for that fold\n",
    "        fold_score = r2_score( y_valid , reg.predict(X_valid) )\n",
    "        \n",
    "        ## keeping in the curve specific to this fold\n",
    "        fold_r2s[i].append( fold_score )\n",
    "        \n",
    "        ## keeping a tally of the average across folds\n",
    "        cross_validated_r2[-1] += fold_score/kf.n_splits\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "bestI = np.argmax(cross_validated_r2)\n",
    "bestLogAlpha = logalphas[bestI] \n",
    "bestR2_valid = cross_validated_r2[bestI]\n",
    "\n",
    "\n",
    "## plotting\n",
    "fig,ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "ax.plot( logalphas, cross_validated_r2 , label='cross-validated r2' )\n",
    "ax.scatter( [bestLogAlpha] , [bestR2_valid]  , c='red')\n",
    "\n",
    "for i,scores in enumerate(fold_r2s):\n",
    "    ax.plot( logalphas , scores , label = f'fold {i}' , linestyle='dotted' )\n",
    "\n",
    "ax.set_xlabel(\"log10( alpha )\")\n",
    "ax.set_ylabel(\"R2\")\n",
    "ax.set_ylim(-0.1,1.1)\n",
    "ax.legend()\n",
    "\n",
    "fig.suptitle(\"best alpha : {:.2f} - cross-validated R2 : {:.2f}\".format(10**bestLogAlpha , bestR2_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935fee7",
   "metadata": {},
   "source": [
    "**micro-exercise**: re-fit a model with the alpha we found and check the performance with the *test* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a272b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c739a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_mini2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d8101",
   "metadata": {},
   "source": [
    "There, you can realize that now, for each possible value of our hyper-parameter we fit and evaluate not 1, but $k$ models, here 4.\n",
    "\n",
    "So, for 200 values of $\\alpha$, that means 200x5 = 1000 models to fit and evaluate.\n",
    "\n",
    "Now, consider that we have other hyper-parameters, such as the type of regularization (L1 or L2),\n",
    "or how we perform scaling, or whether we consider interactions, and now you understand why Machine Learning can quickly become  computationally intensive. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1952a24",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## approach 4 : a \"classical\" ML pipeline <a class=\"anchor\" id=\"linear-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a596e99",
   "metadata": {},
   "source": [
    "We will start back from scratch to recapitulate what we've seen and use scikit-learn's helpful recipes to solve the potato problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## full dataset\n",
    "X = dfTT\n",
    "y = df[ \"Flesh Colour\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1a36a",
   "metadata": {},
   "source": [
    "We start by splitting our data in a train and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train, y_test = train_test_split(X,y , test_size=0.2 )\n",
    "\n",
    "print('train set size:',len(y_train))\n",
    "print(' test set size:',len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85ce62",
   "metadata": {},
   "source": [
    "--- \n",
    "Here comes the question : **\"What should be my test-set size?\"** \n",
    "\n",
    "There is not a single clear-cut answer to this and it depends a lots on the nature of your data and the of the problem you want to solve.\n",
    "\n",
    "\n",
    "On one hand, if you have **too few training samples**: your model will have a hard time finding the patterns in the data and will likily have poor permance.\n",
    "\n",
    "On one hand, if you have **too few testing samples**: the evaluation of your model will be very imprecise.\n",
    "\n",
    "The typical thing you will see around is 80% training / 20% testing, or 75%/25% **BUT**\n",
    "\n",
    "here are some of things you want to keep in mind :\n",
    "\n",
    " * consider the **number of samples** you put in the testing set rather than the proportion\n",
    " * you want the test set to be **representative**: how much samples do you need for that?\n",
    " * what sort of precision would you like to have on the evaluation of your model?\n",
    "     You can use [proportion confidence intervals](https://www.statskingdom.com/proportion-confidence-interval-calculator.html) to help you\n",
    " * **how hard is your problem?** Some problems need a lot of data, and some needs a lot.\n",
    " \n",
    "You can read more on this subject in this [blog post](https://www.r-bloggers.com/2021/01/what-is-a-good-test-set-size/). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9c7fb",
   "metadata": {},
   "source": [
    "Now we train a model while optimizing some hyper-parameters.\n",
    "\n",
    "On top of what we've done before, I add a scaling phase, and test l1 or l2 penalties.\n",
    "\n",
    "Scikit-learn's `GridSearchCV` is useful to explore these more \"complex\" hyper-parameter spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0553575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "## our pipeline will have 2 consecutive steps\n",
    "##   * standard scaling : set mean of each feature at 0 and (optionally) standard dev at 1 \n",
    "##   * linear regression with some regularization\n",
    "pip_reg = Pipeline([('scaler',StandardScaler()),\n",
    "                    ('model',SGDRegressor())])\n",
    "\n",
    "\n",
    "\n",
    "# define the hyperparameters you want to test\n",
    "# with the range over which you want it to be tested.\n",
    "# \n",
    "# They are given in a dictionary with the structure:\n",
    "#      pipelineStep__parameter : [set of values to explore]\n",
    "#                  ^^\n",
    "#                  note the double underscore _\n",
    "grid_values = {'scaler__with_std' : [ True , False ],\n",
    "               'model__penalty':[ 'l1' , 'l2' ],\n",
    "               'model__alpha':np.logspace(-2,2,200)}\n",
    "\n",
    "\n",
    "# Feed the pipeline and set of values to the GridSearchCV with the \n",
    "# score over which the decision should be taken (here, R^2).\n",
    "# and the cross-validation scheme, here the number of fold in a stratified k-fold strategy\n",
    "grid_reg = GridSearchCV(pip_reg, \n",
    "                        param_grid = grid_values, \n",
    "                        scoring='r2', \n",
    "                        cv = 5,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "# When the actual fit happens\n",
    "#  the gridSearchCV object will go through each hyperparameter value combination\n",
    "#  and fit + evaluate each fold, and averages the score across each fold.\n",
    "#\n",
    "#  It then finds the combination that gave the best score and\n",
    "#  use it to re-train a model with the whole train data\n",
    "grid_reg.fit(X_train, y_train)\n",
    "\n",
    "# get the best cross-validated score \n",
    "print(f'Grid best score ({grid_reg.scoring}): {grid_reg.best_score_:.3f}')\n",
    "\n",
    "# print the best parameters\n",
    "print('Grid best parameter :')\n",
    "for k,v in grid_reg.best_params_.items():\n",
    "    print(' {:>20} : {}'.format(k,v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## gridSearch CV fits a new estimator with the best hyperparameter values\n",
    "reg = grid_reg.best_estimator_\n",
    "\n",
    "y_pred = reg.predict( X_train )\n",
    "print(f\"train data R-squared score: { r2_score( y_train , y_pred ) :.2f}\")\n",
    "print(f\"train data mean squared error: { mean_squared_error(  y_train , y_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "y_test_pred = reg.predict( X_test )\n",
    "\n",
    "print(f\" test data R-squared score: { r2_score( y_test , y_test_pred ) :.2f}\")\n",
    "print(f\" test data mean squared error: { mean_squared_error(  y_test , y_test_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter( y_train , y_pred , label = 'training data' )\n",
    "plt.scatter( y_test , y_test_pred , label = 'new data' )\n",
    "plt.xlabel('observed values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db668252",
   "metadata": {},
   "source": [
    "One can also access the best model and it's coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceebce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b94d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## coefficient of the linear model\n",
    "grid_reg.best_estimator_['model'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff6de1",
   "metadata": {},
   "source": [
    "One can also access the CV results for each hyper-parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = pd.DataFrame(grid_reg.cv_results_)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c77dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.lineplot( df_cv , \n",
    "                 x = 'param_model__alpha' , \n",
    "                 y = 'mean_test_score' , \n",
    "                 hue = df_cv.param_model__penalty + ' - std scaling:'+ df_cv.param_scaler__with_std.astype(str) , \n",
    "                )\n",
    "                \n",
    "p.set_xscale(\"log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_introml2024)",
   "language": "python",
   "name": "conda_introml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
