{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will **only talk about classification**. Classifiers associate a transformed set of features to a set of classes (a discrete variable)\n",
    "\n",
    "![class_reg](../images/class_reg.png)\n",
    "\n",
    "In this notebook we are going to focus on distance based classification method (opposed to decision tree for example wich works with threshold more than actual distances). The goal is to give you some insight on 4 different levels:\n",
    "- **Best practices** \n",
    "- **Intuition on the concepts behind those methods**\n",
    "- **How to implement them with Scikitlearn**\n",
    "- **Intuition on the different parameters that your model need but are not trainable (hyperparameters)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from IPython.display import Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 8, 8\n",
    "plt.rc(\"font\", size=14)\n",
    "\n",
    "\n",
    "plt.rc('xtick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('xtick.major', size=8, pad=12)\n",
    "plt.rc('xtick.minor', size=8, pad=12)\n",
    "\n",
    "plt.rc('ytick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('ytick.major', size=8, pad=12)\n",
    "plt.rc('ytick.minor', size=8, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance based classification method\n",
    "## Table Of Content: <a id='toc'></a>\n",
    "\n",
    "* [Meet the data](#data)\n",
    "    * [Binary class dataset : breast cancer](#data-cancer)\n",
    "\n",
    "* [**K-nearest neighbors : introduction to basic routine in ML**](#neighbors)\n",
    "    * [Toy dataset: KNN concepts and hyperparameters](#KN-concepts)\n",
    "    * [a metric to score classification : accuracy](#accuracy)\n",
    "    * [finding the best hyper-parameter set with KNN](#knn-hyper)\n",
    "    * [important considerations: leakage](#metric-leakage)\n",
    "    * [applying all this to the cancer dataset](#knn-cancer)\n",
    "    \n",
    "* [**Logistic regression**](#Logistic-regression)\n",
    "    * [Concepts: what is linear regression and introduction to regularization](#Logistic-regression)\n",
    "    * [Breast cancer dataset](#LR-hyper)\n",
    "    * [imbalanced dataset](#imbalanced)    \n",
    "    \n",
    "* [**Multiclass problems**](#LR-IRIS)\n",
    "    \n",
    "    \n",
    "* [**Exercise: criticizing code**](#exercise-student)\n",
    "* [**Exercise: predicting 10 year coronary heart disease outcome**](#exercise-heart)\n",
    "    \n",
    "\n",
    "* [**Appendices**](#APPENDIX)\n",
    "    * [**Support Vector Machine**](#SVM)\n",
    "    * [**Multiclass dataset : penguins identification**](#data-penguin)\n",
    "    * [**Exercise**](#exo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meet the data <a id='data'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "## Binary class dataset : breast cancer <a id=\"data-cancer\"></a>\n",
    "\n",
    "In the cancer dataset you have 569 tumors for which many features have been measured. The goal is to predict if the tumor is malignant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the breast cancer dataset is integrated in the sklearn library\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "## we reduce the features because otherwise this problem is a bit too easy ;-)\n",
    "m = list( map( lambda x : x.startswith(\"mean \") , data[\"feature_names\"] ) )\n",
    "\n",
    "\n",
    "X_cancer=data['data'][:,m]\n",
    "\n",
    "# for some reason this dataset has encoded 0 for malignant,\n",
    "# which is extremely counter intuitive\n",
    "# So I choose to switch it to the more intuitive order now\n",
    "y_cancer= 1-data['target']\n",
    "\n",
    "#making it into a dataframe\n",
    "df_cancer=pd.DataFrame(X_cancer,\n",
    "    columns=data[\"feature_names\"][m])\n",
    "\n",
    "df_cancer[\"malignant\"]=y_cancer\n",
    "\n",
    "# 0 : benign\n",
    "# 1 : malignant\n",
    "\n",
    "\n",
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer.malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features:\\n')\n",
    "for s in df_cancer.columns:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer.malignant.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there is a slight over-representation of benign tumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer_corr = df_cancer[list(df_cancer.columns)[:-1]].corr()\n",
    "\n",
    "sns.clustermap(df_cancer_corr,figsize=(10,10),z_score=None,row_cluster=True,col_cluster=True,method='ward',cmap='coolwarm',vmax=1,vmin=-1, annot=True, annot_kws={\"size\": 13},cbar_kws={\"label\": 'Pearson\\ncorrelation'})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "# K-nearest neighbors +  some basic routine <a class=\"anchor\" id=\"neighbors\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors is a pretty simple algorithm in terms of concept but it already has few hyperparameters that you should  understand and try to optimize. It will introduce you to some of the very experimental like routine that machine learning is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classifier, k-nearest neighbors works as follow. \n",
    "\n",
    "**First the algorithm simply saves the labels that it is given during the training phase**. \n",
    "\n",
    "Then during the testing phase it takes a testing point and checks its `n_neighbors` nearest neighbors. \n",
    "\n",
    "**If `n_neighbors` nearest neighbors are mostly (in majority) from one label then the tested point will be assigned this label**. \n",
    "\n",
    "The way the `n_neighbors` **nearest neighbors vote** can be either \n",
    "\n",
    " * uniform (every point as the same importance in the vote) or,\n",
    " * distance-based (a point distant to the tested point by a distance d will have a weight of 1/d in the vote).\n",
    "\n",
    "![knn](../images/knn.png)\n",
    "\n",
    "[Image from datacamp tutorials](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "### Toy dataset: KNN concepts and hyperparameters<a class=\"anchor\" id=\"KN-concepts\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make some toy dataset. Here using the sklearn function making blobs for you!!!\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# 120 points, 3 blobs/clusters with some spread=3\n",
    "blob_centers = np.array([[-7,2.5],[6,-6],[8,-3]])\n",
    "blob_stds = [[1,3],[1,3],[1,3]]\n",
    "X_3, y_3 = make_blobs(n_samples = 120, \n",
    "                      centers = blob_centers,\n",
    "                      cluster_std = blob_stds, random_state = 42)\n",
    "\n",
    "\n",
    "#Random_state is here just to be sure that every time you will get the same blobs. \n",
    "# If you change the random_state or do not\n",
    "# specify it then you will get a new plot every time you call the function (random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_3[:,0],X_3[:,1],c=y_3,cmap=plt.cm.coolwarm,edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitting a KNN classifier with sklearn looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create classifier\n",
    "knn = KNeighborsClassifier( n_neighbors= 5 ) ## how many neighbors to consider\n",
    "\n",
    "## fitting classifier\n",
    "knn.fit( X_3 , y_3 )\n",
    "\n",
    "## getting the classes predicted by the classifier:\n",
    "knn.predict( X_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## comparing the prediction and the truth:\n",
    "\n",
    "pd.crosstab( y_3 , knn.predict( X_3 ) , rownames=['truth'] , colnames=['prediction'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the effect of the number of neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import contour_knn # in utils.py we prepared a number of convenience functions \n",
    "fig,axes = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "## 12 neighbors\n",
    "contour_knn(120,X_3,y_3,'uniform', ax=axes[0])\n",
    "\n",
    "## 20 neighbors\n",
    "contour_knn(20,X_3,y_3,'uniform', ax=axes[1])\n",
    "\n",
    "## 1 neighbors\n",
    "contour_knn(1,X_3,y_3,'uniform', ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we varied the number of voters (neighbors) to decide weither a point is from one class or another. \n",
    "\n",
    "You can see how the **boundaries are way more wiggly and attentive to missclassification when the number of neighbors is low**. \n",
    "\n",
    "But you can also imagine that if **new data is added it is likely to be missclassified** with those kind of too specific boundaries. \n",
    "\n",
    "Conversely, when the number of neighbors is very high (120 here), then the **model is not sensitive to point placement**.\n",
    "\n",
    "This is a first example of the **bias variance trade off.**\n",
    "\n",
    "We say that:\n",
    "\n",
    " - the model with k=1 has a **high variance**\n",
    " - the model with k=120 has a **high bias**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Another parameter is **how voting is done**.\n",
    "\n",
    "Above each of the K nearest neighbors contributed equally to the vote.\n",
    "\n",
    "By setting `weights='distance'` the contribution of each neighbor will be weighted by the inverse of its distance to the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "contour_knn(120,X_3,y_3,'distance', ax=axes[0])\n",
    "contour_knn(20,X_3,y_3,'distance', ax=axes[1])\n",
    "contour_knn(1,X_3,y_3,'distance', ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see what changed?\n",
    "\n",
    "So here, connecting this to the previous chapter: K and the weighting, which govern our model bias-variance tradeoff and cannot be set directly from the data are **hyper-parameters** of the KNN algorithm.\n",
    "\n",
    "\n",
    "There are many additional hyper-parameters we could consider as well:\n",
    "\n",
    " * the imputation strategy\n",
    " * the normalization strategy\n",
    " * dimensionality reduction : how many PCA axes do I keep?\n",
    " * choice of the classification algorithm (KNN, logistic regression, ranfom forest, neural network, ...)\n",
    " \n",
    " ...\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "\n",
    "### A metric to score classification : accuracy <a id=\"accuracy\" ></a>\n",
    "\n",
    "One straight forward metric to evaluate our model is the accuracy. Accuracy is only interesting on the test set, even though it can give you some good insight about your model when accuracy is compared between test and training set. \n",
    "\n",
    "But again, the main thing that is going to matter to evaluate your model concern metric evaluated on the test set.\n",
    "\n",
    "\n",
    "Accuracy is defined as follow : $\\frac{TP+TN}{P+N}$\n",
    "\n",
    "* TP : True Positive\n",
    "* TN : True Negative\n",
    "* FP : False Positive\n",
    "* FN : False Negative\n",
    "* P : Positive : $P=TP+FN$\n",
    "* N : Negative : $N=TN+FP$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "truth = [0,0,0,1,1,1]\n",
    "pred  = [0,0,1,0,1,1] # 2 errors out of 6 predictions\n",
    "\n",
    "accuracy_score( truth , pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "\n",
    "### finding the best hyper-parameter set with KNN <a id=\"knn-hyper\" ></a>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement here the procedure we have seen with the potato dataset in the previous chapter :\n",
    "\n",
    "**But** we will have to adapt a few things.\n",
    "\n",
    "\n",
    "First during the train/test split, we use the `stratify` argument to ensure that we have the same proportion of each class in the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train, y_test = train_test_split(X_3,y_3, test_size=0.25 , \n",
    "                                                      stratify=y_3 ) ## stratify  \n",
    "\n",
    "print('train set size:',len(y_train))\n",
    "print(' test set size:',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series( y_train ).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series( y_test ).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we proceed with the hyper-parameter optimization.\n",
    "\n",
    "**But** (there's always a but), we need to change the metric we are optimizing.\n",
    "\n",
    "Before we were optimizing for $R^2$, which appropriate for regression tasks, but now we will switch it for **accuracy**.\n",
    "\n",
    "\n",
    "**micro-exercise:** Change the code below so the gridSearchCV :\n",
    " * operates on accuracy instead of $R^2$. *Hint:* you can look up the [documentation of GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    " * explores hyper-parameters of the KNN *Hint: remember the double _ between step name and parameter name *:\n",
    "     * `weights` : can take values uniform or distance\n",
    "     * `n_neighbors` : can take values between 1 and 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pip_knn = Pipeline([('scaler',StandardScaler()),\n",
    "                    ('model',KNeighborsClassifier())])\n",
    "\n",
    "\n",
    "grid_values = { ... } ## set up the hyper-parameters here\n",
    "\n",
    "\n",
    "grid_knn = GridSearchCV(pip_knn, \n",
    "                        param_grid = grid_values, \n",
    "                        scoring= ... , ## add the proper score here\n",
    "                        cv = 5,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f'Grid best score ({grid_knn.scoring}): {grid_knn.best_score_:.3f}')\n",
    "\n",
    "\n",
    "print('Grid best parameter :')\n",
    "for k,v in grid_knn.best_params_.items():\n",
    "    print(' {:>20} : {}'.format(k,v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "---\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a final model, we can evalutae how it performs on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "## gridSearch CV fits a new estimator with the best hyperparameter values\n",
    "reg = grid_reg.best_estimator_\n",
    "\n",
    "y_pred = reg.predict( X_train )\n",
    "print(f\"train data accuracy: { accuracy_score( y_train , y_pred ) :.2f}\")\n",
    "\n",
    "y_test_pred = reg.predict( X_test )\n",
    "\n",
    "print(f\" test data accuracy: { accuracy_score( y_test , y_test_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "## we can create a confusion matrix:\n",
    "confusion_m = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(confusion_m, annot=True)\n",
    "plt.title('test {} : {:.3f}'.format( grid_knn.scoring , \n",
    "                                    accuracy_score( y_test , y_test_pred ) ))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "It seems that here we get a *perfect* performance on the test set.\n",
    "\n",
    "Why perfect and not slightly below 0 ? \n",
    "\n",
    "This is likely linked to the test set very small size (only 10 points per category) which gives us a very **coarse grained estimate of test accuracy**.\n",
    " \n",
    "> You can read more on this subject in this [blog post](https://www.r-bloggers.com/2021/01/what-is-a-good-test-set-size/). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc) \n",
    "\n",
    "### important considerations: leakage <a class=\"anchor\" id=\"metric-leakage\"></a>\n",
    "\n",
    "The **test set should never be touched until the last step which is the model evaluation**. \n",
    "\n",
    "By doing so we can be confident that our evaluation of the **ability of our model to generalize to new data** is as fair as it can be.\n",
    "\n",
    "We say that no information coming from the test set should leak into the train set. If this is the case, we are biasing our understanding of the generalizability of our model. \n",
    "\n",
    "To avoid **leakage** you should ensure your test set is absent from even the early stages of your pipeline, such as imputation or feature selection (so you have guessed it, most of the operations we have done until now have lead to leakage...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "## applying all this to the cancer dataset <a class=\"anchor\" id=\"knn-cancer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We first divide our data into a train and a test set:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#default if 75% training, 25% testing\n",
    "X_cancer_train, X_cancer_test, y_cancer_train, y_cancer_test = train_test_split(X_cancer, \n",
    "                                                                                y_cancer,\n",
    "                                                                                random_state=7,\n",
    "                                                                                stratify=y_cancer) \n",
    "\n",
    "print(\"number of samples:\")\n",
    "print( f\"\\tfull  dataset: {len(y_cancer)}\" )\n",
    "print( f\"\\ttrain dataset: {len(y_cancer_train)}\" )\n",
    "print( f\"\\ttest  dataset: {len(y_cancer_test)}\" )\n",
    "\n",
    "\n",
    "# train_test_split stratify make sure to split the data such \n",
    "# that the two partitions have similar proportion of each target classes\n",
    "print(\"\\nproportion of malign cancers:\")\n",
    "print( f\"\\tfull  dataset: {y_cancer.mean():.3f}\" )\n",
    "print( f\"\\ttrain dataset: {y_cancer_train.mean():.3f}\" )\n",
    "print( f\"\\ttest  dataset: {y_cancer_test.mean():.3f}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create a basic pipeline : just a KNN classification\n",
    "pipeline_knn=Pipeline([('model',KNeighborsClassifier(n_jobs=-1))])\n",
    "\n",
    "\n",
    "\n",
    "# define the hyperparameters you want to test\n",
    "# with the range over which you want it to be tested. \n",
    "grid_values = {'model__n_neighbors': np.arange(1,100,1),\n",
    "               'model__weights':['uniform','distance']}\n",
    "# the double underscore (__) is used to refer to a step of the pipeline, here the model.\n",
    "\n",
    "\n",
    "\n",
    "#Feed it to the GridSearchCV with the right\n",
    "#score over which the decision should be made\n",
    "\n",
    "grid_knn_acc = GridSearchCV(pipeline_knn, \n",
    "                            param_grid = grid_values, \n",
    "                            scoring='accuracy',\n",
    "                            cv= 10 ,    # 10-fold cross validation\n",
    "                            n_jobs=-1)  # use all available CPUs\n",
    " \n",
    "\n",
    "\n",
    "## now we fit the models \n",
    "grid_knn_acc.fit(X_cancer_train, y_cancer_train)\n",
    "\n",
    "#get the best parameters\n",
    "print('Grid best parameter (max. accuracy):\\n\\t ', grid_knn_acc.best_params_)\n",
    "#get the best score calculated from the training/validation dataset\n",
    "print('Grid best score (cross-validated accuracy): {:.3f}'.format( grid_knn_acc.best_score_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there it is: we have applied cross-validation to explore the space of hyper-parameters and we have gotten hyper-parameter values that provide a good bias-variance trade-off.\n",
    "\n",
    "---\n",
    "\n",
    "But, before we go to the next part, there is one important thing we forgot in this pipeline: \n",
    "like PCA or KMeans, the **KNN algorithm relies on distances** between points.\n",
    "\n",
    "Meaningful distances can only be computed if all features follow a comparable scale, which is not the case in our cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will need to add a scaling step to our `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline_knn=Pipeline([('scale',StandardScaler()), # scaling \n",
    "                       ('model',KNeighborsClassifier(n_jobs=-1))]) # KNN\n",
    "\n",
    "\n",
    "## no new parameters to add for the scaling\n",
    "grid_values = {'model__n_neighbors': np.arange(1,100,1),\n",
    "               'model__weights':['uniform','distance']}\n",
    "\n",
    "grid_knn_acc = GridSearchCV(pipeline_knn, \n",
    "                            param_grid = grid_values, \n",
    "                            scoring='accuracy',\n",
    "                            cv= 10 ,            # 10 fold CV\n",
    "                            n_jobs=-1)          # use all CPUs\n",
    "\n",
    "## fit the models \n",
    "grid_knn_acc.fit(X_cancer_train, y_cancer_train)\n",
    "\n",
    "\n",
    "print('Grid best parameter (max. accuracy):\\n\\t ', grid_knn_acc.best_params_)\n",
    "print('Grid best score (cross-validated accuracy): {:.3f}'.format( grid_knn_acc.best_score_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does the model make more sense, but the cross-validated accuracy is better now.\n",
    "\n",
    "\n",
    "Note that the **proper practice is to include the pre-processing step in the Pipeline**. This:\n",
    "- prevents you from forgetting to apply it in practice\n",
    "- let you **explore hyper-parameters** of your preprocessing with the gridSearchCV (eg, nume of PCA axes to keep)\n",
    "- ensure there is **no leakage from the validation set** during the Cross-Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of hyper-parameters is low, we can plot how they influence the score. \n",
    "\n",
    "This is also useful to see if there are alternative to our best fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame( grid_knn_acc.cv_results_ )\n",
    "#df_res.loc[:, [ 'param_model__n_neighbors', 'param_model__weights',  'mean_test_score', 'std_test_score' ]]\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,7))\n",
    "for W in ['uniform','distance']:\n",
    "    m = df_res.param_model__weights==W\n",
    "    ax.plot( df_res.loc[m , 'param_model__n_neighbors'] , df_res.loc[m , 'mean_test_score'] , \n",
    "               label = W )\n",
    "    ax.fill_between( np.array( df_res.loc[m , 'param_model__n_neighbors'] , dtype = int ), \n",
    "                       np.array( df_res.loc[m , 'mean_test_score']  - df_res.loc[m , 'std_test_score'] ),\n",
    "                       np.array( df_res.loc[m , 'mean_test_score']  + df_res.loc[m , 'std_test_score']) , \n",
    "                    alpha = 0.2 )\n",
    "ax.set_xlabel('number of neighbors')\n",
    "ax.set_ylabel('accuracy')           \n",
    "ax.legend()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that a handful of hyperparameter sets have displayed the same cross-validated performance.\n",
    "\n",
    "The common practice in this case is to use the least complex of the sets, which here corresponds to what the gridSearchCV object proposes.\n",
    "\n",
    "---\n",
    "\n",
    "Once we have found the best set of hyper-parameters using cross-validation, we can evalutae our final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_test_score=grid_knn_acc.score(X_cancer_test,y_cancer_test)\n",
    "\n",
    "print('Grid best parameter (max. accuracy) model on test: ', y_test_score)\n",
    "\n",
    "y_cancer_pred_test = grid_knn_acc.predict(X_cancer_test)\n",
    "\n",
    "confusion_m_cancer = confusion_matrix(y_cancer_test, y_cancer_pred_test)\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(confusion_m_cancer, annot=True)\n",
    "plt.title('test {} : {:.3f}'.format( grid_knn_acc.scoring , y_test_score ))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congrats!!!! You just trained a classical machine learning model using best practice in term of scaling, hyperparameter choice, and data leakage.**\n",
    "\n",
    "You just followed the routine that will be ours in this whole course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can access the best model of the grid search with :\n",
    "grid_knn_acc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might want to save your beautiful model so you can use it later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_knn_acc.best_estimator_\n",
    "\n",
    "import pickle\n",
    "pickle.dump(model, open(\"my_super_model.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you want to load it again\n",
    "\n",
    "loaded_model = pickle.load(open(\"my_super_model.pickle\", 'rb'))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few important points\n",
    "\n",
    " * the GridSearch algorithm is a nice algorithm but it is [not the only option for hyper-parameter space exploration](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    " * K-fold Cross-Validation is ihnerently stochastic, it is common to [repeat it many times](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold)\n",
    " * Accuracy is rarely the most appropriate metric (but more on that later...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "# Logistic regression <a class=\"anchor\" id=\"Logistic-regression\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main hypothesis of logistic regression is that the log odds ratio of the probability that a point is part of a certain class follows a linear combination of the features describing this point.** This is equivalent to the assumption that the two classes are linearly separable and that the log odds ratio increases linearly with the distance from the separating line.\n",
    "\n",
    "Let's say you only have two classes and so your target variable can only take two values $y_{i} \\in\\{1,0\\}$. Let's define $p(\\pmb{x})=P(y=1)$ the probability than your datapoint $\\pmb{x}$ belongs to class 1. Let's also say that you have $M$ features to describe a point $\\{\\pmb{x}_{i}\\}_{i=1,...,M}$. And that you have N points.\n",
    "\n",
    "Then we make the hypothesis that:\n",
    "\n",
    "$log{\\frac{p}{1-p}}=w_{0}+\\Sigma^{M}_{i=1}w_{i}x_{i}=w_{0}+\\pmb{x}\\cdot\\pmb{w}$\n",
    "\n",
    "which translates to\n",
    "\n",
    "$p(\\pmb{x}|y=1)=\\frac{1}{1+e^{-(w_{0}+\\pmb{x}\\cdot\\pmb{w})}}$  \n",
    "\n",
    "\n",
    "![logreg](../images/lr.png)\n",
    "\n",
    "**So the probability that your datapoint is in class 1 is the logistic function (sigmoid) applied to the linear combination of features.** The larger the weights $\\pmb{w}$ the steeper the change between the two classes. The values of $\\pmb{w}$ are stored by the 'coeff_' attribute of the [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) class. $w_{0}$ is stored in 'intercept_'. The function 'decision_function($\\pmb{x}$)' returns $w_{0}+\\pmb{x}\\cdot\\pmb{w}$ and 'predict_proba($\\pmb{x}$)' returns $p(\\pmb{x}|y=1)$ and $p(\\pmb{x}|y=0)$\n",
    "\n",
    "Unlike KNN the logistic regression model provides valuable information on the importance of features for the classification. The bigger the absolute value of the weight $w_{i}$ associated with a feature, the more important this feature is to discriminate between your classes (supposed that features are normalized and mapped onto a similar scale).\n",
    "\n",
    "Fitting a logistic regression model corresponds to optimizing the loss function\n",
    "\n",
    "$\\pmb{w},w_{0}=argmin_{\\pmb{w},w_{0}}\\mathcal{L}(\\pmb{w},w_{0},\\pmb{X},\\pmb{y})$\n",
    "<br><br>\n",
    "$\\mathcal{L}(\\pmb{w},w_{0},\\pmb{X},\\pmb{y})=\\Sigma^{N}_{i=1}y_{i}\\log(p(\\pmb{x}_i|y=1))+(1-y_{i})\\log(p(\\pmb{x}_i|y=0))$\n",
    "\n",
    "So, now the way we get the w is from fitting our distribution of probability that a point is in class 1. If the number of features is high and there is a significant amount of non-informative features, the fitting procedure can become unstable. What happens is that some weights of non-informative features may become very large in order to minimize the loss function on the training data, which may lead to reduced performance on the test data (overfitting).\n",
    "\n",
    "In order to avoid this situation we can add a penalty to the loss function that avoids that weights $w_{i}$ become too large. In ML three such penalties are commonly used:\n",
    "\n",
    "$\\mathcal{L}(\\pmb{w},w_{0},\\pmb{X},\\pmb{y}) + \\frac{1}{C}\\Sigma^{N}_{i=1}|w_{i}|$ , l1 regularization (Lasso) C being the inverse of the weight that you put on that regularization \n",
    "\n",
    "$\\mathcal{L}(\\pmb{w},w_{0},\\pmb{X},\\pmb{y}) + \\frac{1}{C}\\Sigma^{N}_{i=1}w_{i}^{2}$ , l2 regularization (Ridge) \n",
    "\n",
    "$\\mathcal{L}(\\pmb{w},w_{0},\\pmb{X},\\pmb{y}) + \\frac{1}{C}\\Sigma^{N}_{i=1}(\\alpha|w_{i}|+(1-\\alpha)w_{i}^{2})$ , elasticnet\n",
    "\n",
    "For a deeper understanding of those notions :\n",
    "\n",
    " - https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net\n",
    "\n",
    " - https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **L1 (lasso)** regularization produces many zero weights.\n",
    "    It is useful for the case where you suspect that only a few features are informative. \n",
    "* **L2 (ridge)** regularization leads to many features with small, but non-zero weights. \n",
    "\n",
    "\n",
    "The L1 regularization may have more bias, and the L2 more variance. \n",
    "\n",
    "The elasticnet interpolates between these two situations at the cost of having and additional parameter $\\alpha$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array( df_cancer.loc[:,['mean radius']] )\n",
    "x_norm = StandardScaler().fit_transform(x)\n",
    "y = df_cancer['malignant']\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = (14,5) )\n",
    "ax.scatter( x_norm , y , c = y )\n",
    "for alpha in [0.01,0.1,1,10]:\n",
    "    \n",
    "    # this implementation does not take alpha but rather C = 1/alpha\n",
    "    C = 1/alpha\n",
    "    lr = LogisticRegression( penalty = 'l2' , C = C  , solver='liblinear')\n",
    "    lr.fit(x_norm , y)\n",
    "    \n",
    "    proba = lr.predict_proba(np.linspace(x_norm.min()-1,x_norm.max()+1,100).reshape(-1, 1))\n",
    "    ax.plot( np.linspace(x_norm.min()-1,x_norm.max()+1,100) , proba[:,1] , label = 'alpha = {}'.format(alpha) )\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do the fully fledged  GridSearchCV, \n",
    "we do a little demo of the **effect of L1 and L2 regularization on model parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we split in train and valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_cancer_train, \n",
    "                                                      y_cancer_train,\n",
    "                                                      random_state=13245,\n",
    "                                                      test_size=0.5,\n",
    "                                                      stratify=y_cancer_train) \n",
    "\n",
    "## scaling \n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_valid_norm = scaler.fit_transform(X_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list( df_cancer.columns) [:-1]\n",
    "\n",
    "logCs = []\n",
    "\n",
    "coef_dict = {'name' : [],\n",
    "             'val' : [],\n",
    "             'log_C' : []}\n",
    "accuracies = []\n",
    "\n",
    "for C in np.logspace(-3,2,100):\n",
    "\n",
    "    lr = LogisticRegression( penalty = 'l1' , C = C  , solver='liblinear')\n",
    "    lr.fit(X_train_norm , y_train)\n",
    "    \n",
    "    logCs.append(np.log10(C))\n",
    "    accuracies.append( accuracy_score( y_valid , lr.predict(X_valid_norm) ) )\n",
    "    \n",
    "    coef_dict['name'] += list( feature_names )\n",
    "    coef_dict['val'] += list( lr.coef_[0] )\n",
    "    coef_dict['log_C'] += [np.log10(C)]* len(feature_names )\n",
    "\n",
    "coef_df = pd.DataFrame(coef_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestC = logCs[ np.argmax( accuracies ) ]\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (20,10))\n",
    "\n",
    "ax[0].plot(logCs , accuracies)\n",
    "ax[0].set_xlabel(\"log10( C )\")\n",
    "ax[0].set_ylabel(\"validation accuracy\")\n",
    "ax[0].axvline( bestC, color='r', ls = '--' )\n",
    "\n",
    "sns.lineplot( x = 'log_C' , y='val' , hue = 'name' , data= coef_df , ax = ax[1])\n",
    "ax[1].axvline( bestC , color='r', ls = '--' )\n",
    "\n",
    "fig.suptitle(\"logistic regression of cancer data with an L1 regularization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**micro-exercise:** We adapted the code above to generate this plot with an **L2** penalty. How do you interpret the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list( df_cancer.columns) [:-1]\n",
    "\n",
    "logCs = []\n",
    "\n",
    "coef_dict = {'name' : [],\n",
    "             'val' : [],\n",
    "             'log_C' : []}\n",
    "accuracies = []\n",
    "\n",
    "for C in np.logspace(-4,1,100):\n",
    "\n",
    "    lr = LogisticRegression( penalty = 'l2' , C = C  , solver='liblinear')\n",
    "    lr.fit(X_train_norm , y_train)\n",
    "    \n",
    "    logCs.append(np.log10(C))\n",
    "    accuracies.append( accuracy_score( y_valid , lr.predict(X_valid_norm) ) )\n",
    "    \n",
    "    coef_dict['name'] += list( feature_names )\n",
    "    coef_dict['val'] += list( lr.coef_[0] )\n",
    "    coef_dict['log_C'] += [np.log10(C)]* len(feature_names )\n",
    "\n",
    "coef_df = pd.DataFrame(coef_dict)\n",
    "bestC = logCs[ np.argmax( accuracies ) ]\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (20,10))\n",
    "\n",
    "ax[0].plot(logCs , accuracies)\n",
    "ax[0].set_xlabel(\"log10( C )\")\n",
    "ax[0].set_ylabel(\"validation accuracy\")\n",
    "ax[0].axvline( bestC, color='r', ls = '--' )\n",
    "\n",
    "sns.lineplot( x = 'log_C' , y='val' , hue = 'name' , data= coef_df , ax = ax[1])\n",
    "ax[1].axvline( bestC , color='r', ls = '--' )\n",
    "\n",
    "fig.suptitle(\"logistic regression of cancer data with an L2 regularization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## logistic regression on the breast cancer dataset  <a class=\"anchor\" id=\"LR-hyper\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_r=LogisticRegression(solver='liblinear',n_jobs=1) \n",
    "## the liblinear solver supports both L1 and L2\n",
    "# for more details see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "pipeline_lr=Pipeline([('scalar',StandardScaler()), \n",
    "                      ('model',logi_r)])\n",
    "\n",
    "\n",
    "\n",
    "grid_values = {'model__C': np.logspace(-5,2,200),\n",
    "               'model__penalty': ['l1','l2'] }\n",
    "# define the hyperparameters you want to test\n",
    "# with the range over which you want it to be tested.\n",
    "\n",
    "\n",
    "# Feed it to the GridSearchCV with the right\n",
    "# score(here accuracy) over which the decision should be taken\n",
    "grid_lr_acc = GridSearchCV(pipeline_lr, \n",
    "                           param_grid = grid_values, \n",
    "                           scoring='accuracy',\n",
    "                           cv=10, \n",
    "                           n_jobs=-1)\n",
    "\n",
    "\n",
    "## this cell throws a lot of warning, I remove them with the lines under\n",
    "grid_lr_acc.fit(X_cancer_train, y_cancer_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Grid best parameter (max. accuracy): ', grid_lr_acc.best_params_)#get the best parameters\n",
    "print('Grid best score (cross-validated accuracy): {:.4f}'.format( grid_lr_acc.best_score_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now compare this to the best KNN classifier we obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Grid best score (cross-validated accuracy): {:.4f}'.format( grid_knn_acc.best_score_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like cross-validated performance of both models are very similar here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the logistic regression actually discriminates the two classes with the code below. \n",
    "\n",
    "Or said differently : **what can we learn from our model**, apart from pure classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "lr = grid_lr_acc.best_estimator_[1]\n",
    "w=lr.coef_[0]#get the weights\n",
    "\n",
    "featureW = pd.DataFrame( {'feature':df_cancer.columns[:-1],'weight':w} )\n",
    "\n",
    "featureWsorted = featureW.sort_values(by=['weight'] , \n",
    "                                      ascending=False , \n",
    "                                      key=lambda col : col.abs())\n",
    "\n",
    "# get the non-null ones\n",
    "print('Features sorted per importance:')\n",
    "print( featureWsorted.loc[ featureWsorted[\"weight\"] !=0 ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Imbalanced dataset <a class=\"anchor\" id=\"imbalanced\"></a> \n",
    "\n",
    "Let's use the same small example as before, but now instead of 300 sample of each class, imagine we only have 30 of class 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.concatenate( [ np.random.randn(300) , np.random.randn(30)+2 ])\n",
    "y = np.array( [0]*300 + [1]*30 )\n",
    "\n",
    "# do not forget to scale the data\n",
    "X1_norm = StandardScaler().fit_transform(X1.reshape( X1.shape[0] , 1 ))\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize = (14,5) )\n",
    "\n",
    "sns.histplot( x=X1,hue = y , ax =ax [0])\n",
    "\n",
    "\n",
    "ax[1].scatter( X1_norm , y , c = y )\n",
    "\n",
    "for alpha in [0.01,0.1,1,10]:\n",
    "    \n",
    "    # this implementation does not take alpham but rather C = 1/alpha\n",
    "    C = 1/alpha\n",
    "    lr = LogisticRegression( penalty = 'l2' , C = C )\n",
    "    lr.fit(X1_norm , y)\n",
    "    \n",
    "    proba = lr.predict_proba(np.linspace(-2,3,100).reshape(-1, 1))\n",
    "    ax[1].plot( np.linspace(-2,3,100) , proba[:,1] , label = 'alpha = {}'.format(alpha) )\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that now the point where the probability curves for different alpha converge is not 0.5 anymore...\n",
    "\n",
    "Also, the probability says fairly low even at the right end of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = lr.predict(X1_norm)\n",
    "print( f\"Accuracy with a threshold of 0.5 : {accuracy_score(y,y_predicted):.2f}\"  )\n",
    "pd.crosstab( y , y_predicted )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, most of the class 1 samples are miss-classified (22/30), but we still get a very high accuracy...\n",
    "\n",
    "That is because, by contruction, both the **logistic regression and accuracy score do not differentiate False Positive and False Negative**.\n",
    "\n",
    "And the problem gets worse the more imbalance there is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "## RECALL = TP / (TP + FN)\n",
    "\n",
    "recall_list = []\n",
    "acc_list = []\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "## trying many values of imbalance\n",
    "imbalance_list = np.linspace(0,0.99,50)\n",
    "for imbalance in imbalance_list:\n",
    "\n",
    "    ## generating the imbalanced data\n",
    "    n0 = 300\n",
    "    n1 = int( n0 * (1 - imbalance) )\n",
    "    if n1 == 0:\n",
    "        n1 = 1\n",
    "    \n",
    "    X1 = np.concatenate( [ np.random.randn(n0) , np.random.randn(n1)+2 ])\n",
    "    y = np.array( [0]*n0 + [1]*n1 )\n",
    "\n",
    "    X1_norm = StandardScaler().fit_transform(X1.reshape( X1.shape[0] , 1 ))\n",
    "    \n",
    "    \n",
    "    ## fitting the logistic regression\n",
    "    lr = LogisticRegression( penalty = 'l2' , C = 1/alpha )\n",
    "    lr.fit(X1_norm , y)\n",
    "    \n",
    "    y_predicted = lr.predict(X1_norm)\n",
    "\n",
    "    ## computing accuracy and recall\n",
    "    recall_list.append( recall_score( y , y_predicted ) )\n",
    "    acc_list.append( accuracy_score(y,y_predicted) )\n",
    "\n",
    "        \n",
    "fig,ax=plt.subplots(figsize = (10,4))\n",
    "ax.plot( imbalance_list , acc_list , label='accuracy' )\n",
    "ax.plot( imbalance_list , recall_list , label='recall' )\n",
    "ax.set_xlabel(\"imbalance\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not only does the precision get worse, the **accuracy actually gets higher as there is more imbalance!**\n",
    "\n",
    "So the problem here may be 2-fold:\n",
    " * imbalance in our dataset skews the **logistic regression** toward a particular outcome\n",
    " * **accuracy** is not able to differenciate between False Positive and False Negative, and so it is **blind to imbalance**\n",
    "\n",
    "Consequently, the solutions will have to come both from the model, and from the metric we are using.\n",
    "\n",
    "\n",
    "**For the logistic regression**:\n",
    " * we will re-weight sample according to their class frequency, so that they are more important during the fitting.\n",
    " * in sklearn : `LogisticRegression( ... , class_weight='balanced')`\n",
    " \n",
    "<br> \n",
    "\n",
    "**For the metric**, there exists several metrics which are sensitive to imbalance problems. \n",
    "Here we will introduce some of the main ones:\n",
    "\n",
    "- **[balanced accuracy](https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score)**:\n",
    "\n",
    "$$balanced\\_accuracy = 0.5*( \\frac{TP}{TP+FN} + \\frac{TN}{TN+FP} )$$\n",
    "\n",
    "- **[average-precision score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score)**: weighted mean of precisions achieved at each recall threshold\n",
    "\n",
    "\n",
    "Precision : $\\frac{\\bf{TP}}{\\bf{TP}+\\bf{FP}}$\n",
    "\n",
    "Recall : $\\frac{\\bf{TP}}{\\bf{TP}+\\bf{FN}}$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/precision_recall_curve.png\" alt= \"precision-recall curve\" width=\"500px\">\n",
    "\n",
    "- **[ROC AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)**: Area Under the Receiver Operating Characteristic Curve \n",
    "\n",
    "<img src=\"../images/ROC_curve.png\" alt= \"ROC\" width=\"500px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, recall_score , average_precision_score , roc_auc_score\n",
    "\n",
    "\n",
    "def check_imbalance_effect( imbalance_list , class_weight = None):\n",
    "    \n",
    "    recall_list = []\n",
    "    balanced_acc_list = []\n",
    "    acc_list = []\n",
    "    avg_prec_list = []\n",
    "    roc_auc_list = []\n",
    "    \n",
    "    for imbalance in imbalance_list:\n",
    "\n",
    "        n0 = 300\n",
    "        n1 = int( n0 * (1 - imbalance) )\n",
    "        if n1 == 0:\n",
    "            n1 = 1\n",
    "\n",
    "        X1 = np.concatenate( [ np.random.randn(n0) , np.random.randn(n1)+2 ])\n",
    "        y = np.array( [0]*n0 + [1]*n1 )\n",
    "\n",
    "        X1_norm = StandardScaler().fit_transform(X1.reshape( X1.shape[0] , 1 ))\n",
    "\n",
    "        # LR\n",
    "        lr = LogisticRegression( penalty = 'l2' , C = 1 , class_weight=class_weight)\n",
    "        lr.fit(X1_norm , y)\n",
    "\n",
    "        y_predicted = lr.predict(X1_norm)\n",
    "\n",
    "        recall_list.append( recall_score( y , y_predicted )  )\n",
    "        acc_list.append( accuracy_score(y,y_predicted) )\n",
    "        balanced_acc_list.append( balanced_accuracy_score(y,y_predicted) )\n",
    "        avg_prec_list.append( average_precision_score(y,y_predicted) )\n",
    "        roc_auc_list.append( roc_auc_score(y,y_predicted) )\n",
    "\n",
    "    return {'recall':recall_list , \n",
    "            'accuracy':acc_list , \n",
    "            'balanced_accuracy':balanced_acc_list , \n",
    "            'average_precision':avg_prec_list ,\n",
    "            'ROC_AUC':roc_auc_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_list = np.linspace(0,0.99,50)\n",
    "\n",
    "fig,ax=plt.subplots(1,2,figsize = (12,4))\n",
    "\n",
    "for i,class_weight in enumerate([None,'balanced']):\n",
    "\n",
    "    scores = check_imbalance_effect( imbalance_list , class_weight = class_weight)\n",
    "\n",
    "    for scoreName in scores:\n",
    "        ax[i].plot( imbalance_list , scores[scoreName] , \n",
    "                   label=scoreName )\n",
    "    ax[i].set_xlabel(\"imbalance\")\n",
    "    ax[i].set_ylim(0,1)\n",
    "    ax[i].set_title('class_weight={}'.format(class_weight))\n",
    "    ax[i].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, aside from accuracy, the metrics are able to detect an imbalance problem.\n",
    "\n",
    "Setting `class_weight='balanced'` in our logistic regression fixes the imbalance at the level of the model,\n",
    "which is reflected in several metrics, except the **average-precision**.\n",
    "\n",
    "Let's look at that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from matplotlib import gridspec\n",
    "\n",
    "fig=plt.figure(figsize = (12,6))\n",
    "ax0 = plt.subplot2grid((3, 3), (0, 0), colspan=2, rowspan=3)\n",
    "axes = [ plt.subplot2grid((3, 3), (0, 2)) , plt.subplot2grid((3, 3), (1, 2)) , plt.subplot2grid((3, 3), (2, 2)) ]\n",
    "\n",
    "for i,imbalance in enumerate( [0.1,0.5,0.8] ) : \n",
    "\n",
    "    ## setup \n",
    "    n0 , n1 = 300 , max( 1 , int( 300 * (1 - imbalance) ))\n",
    "    \n",
    "    X1 = np.concatenate( [ np.random.randn(n0) , np.random.randn(n1)+2 ])\n",
    "    y = np.array( [0]*n0 + [1]*n1 )\n",
    "    X1_norm = StandardScaler().fit_transform(X1.reshape( X1.shape[0] , 1 ))\n",
    "\n",
    "    # LR\n",
    "    lr = LogisticRegression( penalty = 'l2' , C = 1 , class_weight='balanced')\n",
    "    lr.fit(X1_norm , y)\n",
    "\n",
    "    ## precision-recall curve\n",
    "    y_scores = lr.decision_function(X1_norm)#decision_function gives you the proba for a point to be in\n",
    "    prec, rec, thre = precision_recall_curve(y, y_scores)\n",
    "    ax0.plot( rec, prec , label='imbalance={}'.format(imbalance) )    \n",
    "    \n",
    "    ## confusion matrix\n",
    "    y_pred = lr.predict(X1_norm)\n",
    "    confusion_m = confusion_matrix(y, y_pred)\n",
    "    sns.heatmap(confusion_m, annot=True , ax = axes[i],fmt='.0f', cmap=\"crest\")\n",
    "\n",
    "    axes[i].set_ylabel('True label')\n",
    "    axes[i].set_xlabel('Predicted label')\n",
    "    \n",
    "    print('imbalance={}'.format(imbalance) , \n",
    "          'recall {:.2f}\\tprecision {:.2f}'.format(recall_score(y , y_pred) , precision_score(y , y_pred)) )\n",
    "ax0.legend()\n",
    "ax0.set_xlabel('recall')\n",
    "ax0.set_ylabel('precision')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the **average-precision is still sensitive to imbalanced with balanced class weights** because precision **is a ratio across classes: TP and FP**.\n",
    "\n",
    "In contrasts, the others only use in-labels metrics : TP and FN , TN and FP\n",
    "\n",
    "\n",
    "In practice, how you handle imbalance should depend on the nature of your data and your problem.\n",
    "\n",
    "Is the imbalance you see in your data representative of what it will on new data?\n",
    "\n",
    "Do you care about the type of error you make, or you just want to make as little error as possible ? in the second case, simple accuracy might serve you better\n",
    "\n",
    "Do you have pre-defined costs associated to different type of errors (eg. [DALY](https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158) of having un-needed surgery vs. DALY of not having needed surgery) ? In that case you may want to use these instead of class frequencies in the balancing and scoring.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## micro-exercise: logistic-regression \n",
    "\n",
    "Now that you know about imbalance, adapt the code below to train a logistic regression classifier on the cancer dataset to optimize ROC AUC and properly account for imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_r=LogisticRegression(solver='liblinear',n_jobs=1) \n",
    "\n",
    "pipeline_lr=Pipeline([('scalar',StandardScaler()), \n",
    "                      ('model',logi_r)])\n",
    "\n",
    "\n",
    "\n",
    "grid_values = {'model__C': np.logspace(-5,2,200),\n",
    "               'model__penalty': ['l1','l2'] }\n",
    "\n",
    "grid_lr_acc = GridSearchCV(pipeline_lr, \n",
    "                           param_grid = grid_values, \n",
    "                           scoring='accuracy',\n",
    "                           cv=10, \n",
    "                           n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_lr_acc.fit(X_cancer_train, y_cancer_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/solution_02_ME3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## Multiclass problems <a class=\"anchor\" id=\"LR-IRIS\"></a>\n",
    "\n",
    "\n",
    "Many classification problems involve more than 2 categories. \n",
    "\n",
    "For example, remember the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "Xi,yi = load_iris(return_X_y = True , as_frame = True)\n",
    "\n",
    "## simple preprocessing pipeline : scale then PCA\n",
    "iris_prepro = Pipeline([('scalar',StandardScaler()),('PCA',PCA())])\n",
    "iris_prepro.fit(Xi,yi)\n",
    "\n",
    "X_pca = iris_prepro.transform(Xi)\n",
    "sns.scatterplot( x = X_pca[:,0] , y = X_pca[:,1] , hue = yi.astype(str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to train a Logistic regression, optimizing for ROC AUC, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr=Pipeline([('scalar',StandardScaler()),\n",
    "                     ('model',LogisticRegression(class_weight='balanced'))])\n",
    "\n",
    "#hyper parameter space :\n",
    "grid_values = {'model__C': np.logspace(-2,1,50),\n",
    "               'model__penalty':['l1','l2'],\n",
    "               'model__solver':['liblinear']}\n",
    "\n",
    "# note the 'roc_auc_ovr' score!\n",
    "grid_iris = GridSearchCV(pipeline_lr, \n",
    "                             param_grid = grid_values, \n",
    "                             scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid_iris.fit(Xi, yi)\n",
    "\n",
    "print('Grid best parameter (max. roc_auc):')\n",
    "print( '\\t' + '\\n\\t'.join([str(x) for x in grid_iris.best_params_.items()]))\n",
    "print('Grid best score (roc_auc): ', grid_iris.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR, like many other models, is, originally, really about classifying 0 and 1, so binary.\n",
    "\n",
    "Of course all of that can still be applied to a multi-class classification, with a little bit of adaptation.\n",
    "\n",
    "There are many different ways of tackling the problem, which end up being combinations of these 3 elements :\n",
    "\n",
    "* Either you treat the problem as **one class vs one class** (ie, you re-encode your data)\n",
    "* Or you treat the problem as a **one class vs the rest** : you subdivide the problem into as many problem as there are classes\n",
    "* You change your loss function to a **multinomial one**.\n",
    "\n",
    "In any case you need to decide how you are going to **agglomerate those different metrics** (different ROC curves for example):\n",
    "\n",
    " * **micro average** : pull all raw numbers together (eg. number of FP, TP), group them and then calculate your overall statistic (eg. TPR)\n",
    " * **macro average** : calculate each statistics separately and then do the average.\n",
    "\n",
    "Think about the differences induced by those metrics. Why should you use one more than the other? Or maybe you should always use all of them?\n",
    "\n",
    "> Spoiler: is has to do with overall separability and balance between the different class.\n",
    "\n",
    "Previous versions of scikit-learn gave multiple options regarding how to handle multiclass cases, but now they only use the multinomial implementation. \n",
    "\n",
    "Regardless, other methods than the logistic regression still have various option, so it pays to be aware of this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import contour_lr\n",
    "#multinomial implementation \n",
    "contour_lr('l2',X_3,y_3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## more regularization\n",
    "contour_lr('l2',X_3,y_3,10**-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_lr=Pipeline([('scalar',StandardScaler()),\n",
    "                     ('model',LogisticRegression(class_weight='balanced'))])\n",
    "\n",
    "#hyper parameter space :\n",
    "grid_values = {'model__C': np.logspace(-2,1,50),\n",
    "               'model__penalty':['l1','l2'],\n",
    "               'model__solver':['liblinear']}\n",
    "\n",
    "# note the 'roc_auc_ovr' score!\n",
    "grid_iris = GridSearchCV(pipeline_lr, \n",
    "                             param_grid = grid_values, \n",
    "                             scoring='roc_auc_ovr_weighted',n_jobs=-1)\n",
    "\n",
    "grid_iris.fit(Xi, yi)\n",
    "\n",
    "print('Grid best parameter (max. roc_auc_ovr_weighted):')\n",
    "print( '\\t' + '\\n\\t'.join([str(x) for x in grid_iris.best_params_.items()]))\n",
    "print('Grid best score (roc_auc_ovr_weighted): ', grid_iris.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "# Exercise: criticizing code <a class=\"anchor\" id=\"exercise-student\"></a>\n",
    "\n",
    "Here, rather than code yourself the analysis, we want you to take a look at the following analysis and\n",
    "\n",
    "1. list the potential methodological issues (you should see at least 3)\n",
    "2. propose alternative to follow best practices.\n",
    "\n",
    "Implementing these best practices can be done, as a form of extra question if you have the time.\n",
    "\n",
    "> The goal is not performance here, we want to focus on the methodological problems. \n",
    "\n",
    "---\n",
    "\n",
    "**The data:**\n",
    "\n",
    "This [student dataset from kaggle](https://www.kaggle.com/datasets/devansodariya/student-performance-data/data) was obtained in a survey of  395 students' math course in secondary school.\n",
    "\n",
    "The dataset contains various features about the student socio-economical status and relationship to studies,\n",
    "like the student age, and gender, their family size, parents occupation, whether they have received paid tutoring, internet access or alcohol consumption...\n",
    "\n",
    "We are not going to focus on the feature interpretation, here.\n",
    "\n",
    "The goal is to predict if the student got a passing grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student = pd.read_csv(\"../data/student.csv\")\n",
    "df_student.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NB: there are no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_student.drop(columns = ['passing'])\n",
    "y = df_student['passing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One Hot Encoding of all the categorical features\n",
    "XOH = pd.get_dummies(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XOH,y,stratify=y, random_state=42)\n",
    "\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# If I don't increase the number of iterations I get a warning :-(\n",
    "lr = LogisticRegression(max_iter=10**3)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print( \"Logistic regression accuracy on test set:\" , accuracy_score( y_test , lr.predict( X_test ) ) )\n",
    "print( \"KNN accuracy on test set:\" , accuracy_score( y_test , knn.predict( X_test ) ) )\n",
    "\n",
    "print(\"the logistic regression gives better performance, so we use this as our final model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-20 solutions/solution_03_student.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposition of code solving the problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 21-43 solutions/solution_03_student.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 44-69 solutions/solution_03_student.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 70-88 solutions/solution_03_student.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 89- solutions/solution_03_student.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "# Exercise: predicting 10 year coronary heart disease outcome <a class=\"anchor\" id=\"exercise-heart\"></a>\n",
    "\n",
    "The [framingham dataset](https://datacatalog.med.nyu.edu/dataset/10046) links some patient features to their risk to develop a heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_heart=pd.read_csv('../data/framingham.csv')\n",
    "print(\"{} samples - {} features\".format(*(df_heart.shape)))\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##separation in X and y\n",
    "X_heart = df_heart.drop( columns = \"TenYearCHD\" )\n",
    "y_heart = df_heart[ \"TenYearCHD\" ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Build and optimize a classifier to predict the column `'TenYearCHD'` (dependent variable : ten year risk of coronary heart disease).\n",
    " * use a KNN classifier or Logistic regression, ideally, compare them and choose the best.\n",
    " * this dataset has some missing values, decide how you want to handle them.\n",
    " * which metric do you want to use here?\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "**Hint :** before, your grid_param was a dictionnary, and it was specific to a particular model since parameters are model dependant. \n",
    "To adapt to multiple models, make a list of `grid_params` where each instance of the list is a dictionnary of parameters specific to the model, you want to try.\n",
    "*Example:*\n",
    "``` python\n",
    "grid_param = [\n",
    "                {\"classifier\": [KNeighborsClassifier()],\n",
    "                 \"classifier__n_neighbors\": np.arange(1,30,1),\n",
    "                 },\n",
    "                {\"classifier\": [LogisticRegression()],\n",
    "                 \"classifier__penalty\": ['l2','l1'],\n",
    "                 }]\n",
    "```\n",
    "\n",
    "***\n",
    "This is not an easy problem. **Do not despair if you do not get great performance out of your model, but focus on respecting best practices** (avoid leakage, account for imbalance,...)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import and minimalist EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-19 solutions/solution_03_heart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 20-29 solutions/solution_03_heart.py\n",
    "## splitting train and test\n",
    "X_heart_train, X_heart_test, y_heart_train, y_heart_test = train_test_split(X_heart,y_heart,\n",
    "                                                                            stratify=y_heart)\n",
    "#stratify is here to make sure that you split keeping the repartition of labels unaffected\n",
    "\n",
    "print(f\"full : {sum(y_heart)} CHD / {len(y_heart)} samples\")\n",
    "print(f\"train: {sum(y_heart_train)} CHD / {len(y_heart_train)} samples\")\n",
    "print(f\"test : {sum(y_heart_test)} CHD / {len(y_heart_test)} samples\")\n",
    "print(\"\\n***\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the pipeline and grid, and fitting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 30-55 solutions/solution_03_heart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 58-87 solutions/solution_03_heart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting a baseline for understanding the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 88-100 solutions/solution_03_heart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking feature importance to learn more about the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 101-115 solutions/solution_03_heart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "\n",
    "# Appendices <a class=\"anchor\" id=\"APPENDIX\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) <a class=\"anchor\" id=\"SVM\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The basic principle of SVM is pretty simple. SVM aims at finding the 'good' threshold (hyperplane) to separate data from different classes. Conceptually it is very different from logistic regression where you maximize the log likelihood of the log odds function. **With SVM you really look for an hyperplane that separates data and that's it : there is no underlying hypothesis about probability distribution or anything else. It is very geometrical.**\n",
    "\n",
    "So what's a good threshold? Again it is going to depend on the metric you are interested in. But at least a good threshold should be linked to this biais variance trade off in the sens that it should offer flexibility to your model.\n",
    "\n",
    "You can imagine that there is a quite a lot of hyperplanes separating data in your training set. You could stick your threshold right where the class 0 point closest to class 1 lies. But in that case it will be very far from the other class 0 points, which can be a problem. **You could decide that your threshold should be right between the two closest extreme of your classes but that is going to be very sensitive to missclassified data or extreme events... Those points choosen as a reference to put your threshold are called support vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10815657)\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,7))\n",
    "\n",
    "# case 1 \n",
    "norm1=0.2*np.random.randn(100)-2\n",
    "norm2=0.8*np.random.randn(100)+2.5\n",
    "\n",
    "# case 2\n",
    "cauch=0.8*np.random.standard_cauchy(10)-2\n",
    "norm=1*np.random.randn(100)+2.5\n",
    "\n",
    "for i,Ds in enumerate( [ (norm1,norm2) , (cauch,norm) ] ):\n",
    "\n",
    "    ax[i].plot(Ds[0],[1]*len(Ds[0]),'ro',markersize=10)\n",
    "    ax[i].plot(Ds[1],[1]*len(Ds[1]),'bo')\n",
    "\n",
    "    min2 = min( Ds[1] )\n",
    "    max1 = max( Ds[0] )\n",
    "    \n",
    "    ax[i].axvline( min2 , color='k', linestyle='--', label='defined by the most extreme blue point')\n",
    "    ax[i].axvline( (min2 +max1)/2,color='k',linestyle='-.',label='middle of extreme of two classes')\n",
    "    ax[i].legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, once again, you are confronted to a compromise. You should place your threshold somwhere that is globally best even though that would mean some miss-classification. We are back to our regularization problem and of course **Support vector machine has a regularization parameter : C**. The game now becomes placing your threshold right in the middle of points (support vectors) from  each classes that you have \\\"chosen\\\" to be general points for decision making : **they don't need to be the two closest points from different classes anymore. They need to be points where your hyperplane makes the least error differentiating classes.**\n",
    "\n",
    "\n",
    "![svm_margin](../images/SVM_margin.png)\n",
    "\n",
    "Image source : image by wikipedia user Larhmam, distributed under a [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/deed.en).\n",
    "\n",
    "\n",
    "So you want to maximize the margin separating the two classes. This margin is $\\frac{2}{||\\pmb{w}||}$. So you want to minimize $||\\pmb{w}||$. The SVM loss function we want to minimize with respect to $\\pmb{w}$ and $b$ is:\n",
    "\n",
    "$C\\cdot\\Sigma^{N}_{i=1}\\zeta_i + \\frac{1}{2}||\\pmb{w}||^{2}$ subject to $\\zeta_i \\ge 0$ and $y_{i}(w^{T}x_{i}-b) \\ge 1-\\zeta_i$, where $\\zeta_i = \\Sigma^{N}_{i=1}max(0,1-y_{i}(\\pmb{w}\\cdot\\pmb{x}_i-b))$\n",
    " * $y_i$ is $-1$ or $1$ depending on the class of the point $i$\n",
    " * the class of point $\\pmb{x}$ is determined by the SVM using the sign of $(\\pmb{w}\\cdot\\pmb{x}-b)$ (ie, on which side of the $(\\pmb{w}\\cdot\\pmb{x}-b)$ hyperplane we are).\n",
    "\n",
    "\n",
    "\n",
    "Note that you could also use a L1 regularization but it is not implemented in the function we are going to use.\n",
    "\n",
    "Indeed if most of the data points are well separated in term of class on each side of the hyperplane then\n",
    "* most of the time $y_{k}(w^{T}x_{k}-b) \\geq 1$ and so $max(0,1-y_{k}(w^{T}x_{k}-b)=0$ (that's good for minimizing our loss function), \n",
    "* and a few times $y_{k}(w^{T}x_{k}-b) \\leq -1$ and so $max(0,1-y_{k}(w^{T}x_{k}-b) \\geq 2$ (which is polluting our minimization of the loss function).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can see that there is a [dot product](https://en.wikipedia.org/wiki/Dot_product) involved : in the case of a linear hyperplane this dot product is just the cartesian dot product that you probably use all the time. It allows you to calculate distances between points in that cartesian space or between points and hyperplanes. But you might be familiar with other scalar product : like for example when you proceed to a Fourier decomposition of a function. This particular scalar product acts on functions and so is not really of interest for us... But others exist.\n",
    "\n",
    "**So in principle you could use other definitions of distance between points to answer that classification question**. This is what non-linear SVM does and this is why you can choose different so called kernels as hyperparameters as we will see below :\n",
    "\n",
    "$\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}}$ : cartesian\n",
    "\n",
    "$(\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}})^{d}$ : polynomial degree d\n",
    "\n",
    "$exp(-\\gamma||\\overrightarrow{x_{i}}-\\overrightarrow{x_{j}}||^{2})$ : gaussian radial basis\n",
    "\n",
    "$tanh(\\kappa\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}}+c)$ : hyperbolic tangent\n",
    "\n",
    "**This is really powerful for classification but going non-linear by using a kernel trick prevents you from interpreting how your features are massaged to create this classifier... So, if you want interpretability and do science rather than engineering : keep it linear.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3d_svm](../images/3d_svm.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data : linear kernel <a class=\"anchor\" id=\"SVM-linear\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import contour_SVM\n",
    "from sklearn.datasets import make_blobs\n",
    "X_toy_2, y_toy_2 = make_blobs(n_samples=120, centers=2,cluster_std=3, random_state=6)\n",
    "\n",
    "#2 classes\n",
    "#parameters:X,y,C,kernel,degree for polynomial kernel,gamma for radial kernel,multi class strategy\n",
    "contour_SVM(X_toy_2,y_toy_2,c=1,ker='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 classes\n",
    "contour_SVM(X_toy_2,y_toy_2,c=0.01,ker='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing C to 0.01 reduces the cost of misclassification and makes the margin larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's put very little regularization\n",
    "contour_SVM(X_toy_2,y_toy_2,c=1000,ker='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse is true. Increasing C does not make a big difference to C=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data non linear kernel <a class=\"anchor\" id=\"SVM-nonlinear\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's change the scalar product and the notion of distance by using a rbf kernel\n",
    "#Also we did'nt put much of regularization\n",
    "#parameters:X,y,C,kernel,degree for polynomial kernel,gamma for radial kernel,multi class strategy\n",
    "contour_SVM(X_3,y_3,c=1,ker='rbf',gam=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we changed the kernel from linear to Gaussian radial basis. See how the classification boundaries follow local class density levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even less regularization\n",
    "contour_SVM(X_3,y_3,c=1,ker='rbf',gam=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we greatly reduce the standard deviation on the gaussian model (we increased $\\gamma$). See how now the boundaries are really concentrated around each point? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more regularization\n",
    "contour_SVM(X_3,y_3,c=1,ker='rbf',gam=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversely by expanding the variance in the gaussian we end up with larger chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#little regularization but a polynomial kernel of degree 3 this time\n",
    "contour_SVM(X_3,y_3,c=1,ker='poly',deg=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, above we tried a degree 3 polynomial kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer dataset <a class=\"anchor\" id=\"SVM-hyperparameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_cancer_train)\n",
    "X_cancer_train_scaled = scaler.transform(X_cancer_train)\n",
    "X_cancer_test_scaled = scaler.transform(X_cancer_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "C_range=np.logspace(-2,3,50)\n",
    "\n",
    "\n",
    "scores_test=[]\n",
    "scores_train=[]\n",
    "\n",
    "for k in C_range: \n",
    "    SVM_r = svm.SVC(C=k, kernel='linear', \n",
    "                    class_weight='balanced', probability=True)\n",
    "    SVM_r.fit(X_cancer_train_scaled,y_cancer_train)\n",
    "\n",
    "    y_train_prob = SVM_r.predict_proba(X_cancer_train_scaled)\n",
    "    y_test_prob = SVM_r.predict_proba(X_cancer_test_scaled)\n",
    "\n",
    "    train_roc_auc_score=roc_auc_score(y_cancer_train,y_train_prob[:, 1])\n",
    "    test_roc_auc_score=roc_auc_score(y_cancer_test,y_test_prob[:, 1])\n",
    "\n",
    "    scores_test.append(test_roc_auc_score)\n",
    "    scores_train.append(train_roc_auc_score)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlabel('inverse of the l2 weight')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.plot(C_range, scores_train,'k-',linewidth=5,label='train')\n",
    "plt.plot(C_range, scores_test,'r-',linewidth=5,label='test')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_range=np.logspace(-3,3,50)\n",
    "\n",
    "scores_test=[]\n",
    "scores_train=[]\n",
    "\n",
    "for k in gamma_range: \n",
    "    SVM_r = svm.SVC(C=k, kernel='rbf', gamma=k ,\n",
    "                    class_weight='balanced', probability=True)\n",
    "    SVM_r.fit(X_cancer_train_scaled,y_cancer_train)\n",
    "\n",
    "    y_train_prob = SVM_r.predict_proba(X_cancer_train_scaled)\n",
    "    y_test_prob = SVM_r.predict_proba(X_cancer_test_scaled)\n",
    "\n",
    "    train_roc_auc_score=roc_auc_score(y_cancer_train,y_train_prob[:, 1])\n",
    "    test_roc_auc_score=roc_auc_score(y_cancer_test,y_test_prob[:, 1])\n",
    "\n",
    "    scores_test.append(test_roc_auc_score)\n",
    "    scores_train.append(train_roc_auc_score)\n",
    "    \n",
    "plt.figure()\n",
    "plt.xlabel('$\\gamma$ in rbf model')\n",
    "plt.ylabel('ROIC AUC')\n",
    "plt.plot(gamma_range, scores_train,'k-',linewidth=5,label='train')\n",
    "plt.plot(gamma_range, scores_test,'r-',linewidth=5,label='test')\n",
    "#plt.xticks([0,20,40,60,80,100])\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_range=np.arange(0,10,1)\n",
    "\n",
    "scores_test=[]\n",
    "scores_train=[]\n",
    "\n",
    "for k in degree_range: \n",
    "    SVM_r = svm.SVC(C=1, kernel='poly', degree=k, \n",
    "                    probability=True, class_weight='balanced')\n",
    "    SVM_r.fit(X_cancer_train_scaled,y_cancer_train)\n",
    "    y_train_prob = SVM_r.predict_proba(X_cancer_train_scaled)\n",
    "    y_test_prob = SVM_r.predict_proba(X_cancer_test_scaled)\n",
    "\n",
    "    train_roc_auc_score=roc_auc_score(y_cancer_train,y_train_prob[:, 1])\n",
    "    test_roc_auc_score=roc_auc_score(y_cancer_test,y_test_prob[:, 1])\n",
    "\n",
    "    scores_test.append(test_roc_auc_score)\n",
    "    scores_train.append(train_roc_auc_score)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('degree in poly model')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.plot(degree_range, scores_train,'k-',linewidth=5,label='train')\n",
    "plt.plot(degree_range, scores_test,'r-',linewidth=5,label='test')\n",
    "#plt.xticks([0,20,40,60,80,100])\n",
    "#plt.xscale('log')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those 3 hyperparameters show the usual overfitting behaviour when they are increased : monotonous increase of the accuracy on the train function whereas there is a maximum accuracy for the test function after which accuracy decreases with increasing hyperpaprameter value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get the weights from the SVM just use `coef_` again but it will only work if you are in the context of a linear SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String kernels <a class=\"anchor\" id=\"SVM-kernels\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paragraph illustrates how to use a SVM kernel not present in sklearn.svm. The following code uses the string kernel from [Hilmarsson et al. BioRxiv, 2021](https://doi.org/10.1101/2021.09.19.460980) in order to classify genomic sequences (here we use toy data based on patterns from real human samples). \n",
    "\n",
    "![Figure 2 of Hilmarsson et al. BioRxiv, 2021 : illustration of the string kernel computation](../images/stringKernel.png)\n",
    "\n",
    "The code is copied from https://github.com/weekend37/string-kernels/blob/master/example.ipynb. More interesting examples can be found in the paper. First let's install the stringkernel package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to install string-kernel if that is not already done\n",
    "!python3 -m pip install string-kernels --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "data_path = \"../data\"\n",
    "\n",
    "samples_train = np.load(os.path.join(data_path, \"samples_train\"+\".npy\"))\n",
    "samples_validation = np.load(os.path.join(data_path, \"samples_validation\"+\".npy\"))\n",
    "\n",
    "ancestry_train = np.load(os.path.join(data_path, \"ancestry_train\"+\".npy\"))\n",
    "ancestry_validation = np.load(os.path.join(data_path, \"ancestry_validation\"+\".npy\"))\n",
    "\n",
    "reference = np.load(os.path.join(data_path, \"reference\"+\".npy\"))\n",
    "populations = np.load(os.path.join(data_path, \"populations\"+\".npy\"))\n",
    "\n",
    "print(\"number of training samples:\", len(samples_train))\n",
    "print(\"number of validation samples:\", len(samples_validation))\n",
    "print(\"Sequence length:\", len(reference))\n",
    "print(\"Number of ancestries:\", len(populations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data matrix consists of arrays of nucleotides (rows), where each columns corresponds to a sequence position and each row to a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the standart SVM with RBF we need to turn the sequences into numeric values. For this example, we just store the information whether the sequences are the same (0) or different (1) from the reference sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (samples_train != reference).astype(int)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringkernels.utils import plot_label_distribution\n",
    "\n",
    "plot_label_distribution(ancestry_train)\n",
    "np.unique(ancestry_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels need to be turned into numbers in order to use the SVC class with RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(ancestry_train)\n",
    "y_train = le.transform(ancestry_train)\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = (samples_validation != reference).astype(int)\n",
    "y_val = le.transform(ancestry_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_model = svm.SVC(kernel='rbf')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_val)\n",
    "svm_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy: {}%\".format(np.round(svm_accuracy*100, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SVC with the string kernel we encountered in the lecture, we do not need the make the features numeric, but we can pass the string as character arrays directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringkernels.kernels import string_kernel\n",
    "\n",
    "svm_sk_model = svm.SVC(kernel=string_kernel())\n",
    "svm_sk_model.fit(samples_train, y_train)\n",
    "y_pred = svm_sk_model.predict(samples_validation)\n",
    "svm_sk_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy: {}%\".format(np.round(svm_sk_accuracy*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringkernels.utils import plot_accuracies\n",
    "\n",
    "plot_accuracies({\n",
    "    \"SVM\\n RBF-Kernel\": svm_accuracy,\n",
    "    \"SVM\\n String-Kernel\": svm_sk_accuracy,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "## Annex : Multiclass dataset : penguins identification <a id='data-penguin'></a>\n",
    "\n",
    "This dataset consists in data from 344 penguins grouped in three species :\n",
    "\n",
    "![penguins](../images/lter_penguins.png)\n",
    "*Artwork by @allison_horst*\n",
    "\n",
    "[dataset source](https://allisonhorst.github.io/palmerpenguins/)\n",
    "\n",
    "\n",
    "Among the columns present in there, we drop the ones, like `StudyID` or `sampleNumber` to keep only columns of potential interest for the task at hand :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguin = pd.read_csv( \"../data/horst2020_palmerpenguins_raw.csv\" )\n",
    "\n",
    "kept_columns = [ 'Species',  'Clutch Completion', 'Culmen Length (mm)',\n",
    "       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n",
    "       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n",
    "\n",
    "\n",
    "## keep the nice doodle color code!\n",
    "penguin_colors = {'Adelie':'#FF8C00',\n",
    "                  'Chinstrap':\"#A020F0\",\n",
    "                  'Gentoo':\"#008B8B\"}\n",
    "\n",
    "\n",
    "df_penguin = df_penguin.loc[: , kept_columns ]\n",
    "# keep only the first word of the species field. We do not need the latin name here\n",
    "df_penguin.Species = df_penguin.Species.apply(lambda x: x.partition(' ')[0])\n",
    "df_penguin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns correspond to :\n",
    "\n",
    "* **Species** : the penguin species\n",
    "* **Clutch Completion** : if the study nest ws observed with a full clutch, i.e., 2 eggs\n",
    "* **Culmen Length** : length of the dorsal ridge of a bird's bill (millimeters)\n",
    "* **Culmen Depth** : the depth of the dorsal ridge of a bird's bill (millimeters)\n",
    "* **Flipper Length** : the length of a bird's flipper (millimeters)\n",
    "* **Body Mass** : the penguin body mass (grams)\n",
    "* **Sex** : the sex of the animal\n",
    "* **Delta 15 N** : measure of the ratio of stable isotopes 15N:14N\n",
    "* **Delta 13 C** : measure of the ratio of stable isotopes 13C:12C\n",
    "\n",
    "You can see there are some NAs. Let's throw away samples with Nas (only 20 of them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = df_penguin.isnull().sum(axis=1) < 1\n",
    "\n",
    "df_penguin = df_penguin.loc[K,:]\n",
    "df_penguin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguin.Species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finally, we separate the target variable (Species) from the rest\n",
    "y_penguin = df_penguin.Species\n",
    "\n",
    "# here note the usage of pd.get_dummies to one-hot encode categorical data!\n",
    "X_penguin = pd.get_dummies(df_penguin.drop( columns=\"Species\" ), drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_penguin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the structure of the data by plotting the correlation between the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguin_corr = df_penguin.corr(numeric_only=True)\n",
    "\n",
    "sns.clustermap(df_penguin_corr,\n",
    "               figsize=(8,8),\n",
    "               z_score=None,\n",
    "               row_cluster=True,\n",
    "               col_cluster=True,\n",
    "               method='ward',\n",
    "               cmap='coolwarm',vmax=1,vmin=-1, \n",
    "               annot=True, annot_kws={\"size\": 13},cbar_kws={\"label\": 'Pearson\\ncorrelation'})\n",
    "## sns allows you to do a hierarchical clustering that simply\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above pearson correlation matrix is highly informational. \n",
    "\n",
    "It allows you to see that **some variables are probably redundant** and so you don't need to have all of them, hence reducing the complexity of your model. \n",
    "\n",
    "Moreover it is usually bad to have highly correlated variables in your model since it is making it unstable and less interpretable. \n",
    "\n",
    "**micro-exercise :** what could we use to handle/mitigate this problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "# Exercise <a class=\"anchor\" id=\"exo\"></a>\n",
    "\n",
    "\n",
    "Using either the **cancer or the penguin dataset** compare and find the 'best' classifier. What do you learn from those classifiers? \n",
    "\n",
    "Don't forget that you have also seen stuff about preprocessing.\n",
    "\n",
    "Don't go too crazy as it can be time consuming: limit the number of parameter values you are testing for.\n",
    "\n",
    "**Hint :** before, your grid_param was a dictionnary, and it was specific to a particular model since parameters are model dependant. \n",
    "To adapt to multiple models, make a list of `grid_params` where each instance of the list is a dictionnary of parameters specific to the model, you want to try.\n",
    "*Example:*\n",
    "``` python\n",
    "grid_param = [\n",
    "                {\"classifier\": [KNeighborsClassifier()],\n",
    "                 \"classifier__n_neighbors\": np.arange(1,30,1),\n",
    "                 },\n",
    "                {\"classifier\": [LogisticRegression()],\n",
    "                 \"classifier__penalty\": ['l2','l1'],\n",
    "                 }]\n",
    "```\n",
    "\n",
    "**Additionnal question** : How would you work with your PCA data and incorporate it to your model?\n",
    "\n",
    "*Hint:* \n",
    " * Don't forget that PCA needs a centering and a scaling step...\n",
    " * Adding PCA can be quite time consuming too. Try a small number of pca components first with other hyperparameters centered around what you found before!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PENGUIN CORRECTION\n",
    "\n",
    "setup the pipeline and fit it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r -24 solutions/solution_02_penguin.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reporting best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 25- solutions/solution_02_penguin.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CANCER CORRECTION \n",
    "\n",
    "build and fit the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r -26 solutions/solution_02_cancer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 28-45 solutions/solution_02_cancer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot a roc curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 46-47 solutions/solution_02_cancer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative with PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 51-89 solutions/solution_02_cancer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 90-104 solutions/solution_02_cancer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 106- solutions/solution_02_cancer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_introml2024)",
   "language": "python",
   "name": "conda_introml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
